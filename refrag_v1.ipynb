{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a55a05",
   "metadata": {},
   "source": [
    "# REFRAG v1: Training and Inference on Google Colab\n",
    "\n",
    "This notebook provides a complete implementation of **REFRAG (REpresentation For RAG)** - a novel efficient decoding framework for Retrieval-Augmented Generation.\n",
    "\n",
    "## Key Features\n",
    "- **30.85x faster** Time-To-First-Token (TTFT) compared to standard RAG\n",
    "- **16x larger context** with same memory budget\n",
    "- **No perplexity loss** - maintains generation quality\n",
    "\n",
    "## Training Pipeline\n",
    "1. **Phase A (CPT-Recon)**: Reconstruction curriculum - trains encoder + projector\n",
    "2. **Phase B (CPT-Next)**: Next-paragraph prediction - fine-tunes full model\n",
    "3. **Policy Training**: REINFORCE-based selective expansion policy\n",
    "\n",
    "## Requirements\n",
    "- Google Colab with GPU runtime (T4/V100/A100)\n",
    "- ~8GB+ GPU memory recommended\n",
    "\n",
    "---\n",
    "\n",
    "**Paper**: REFRAG: Rethinking RAG based Decoding (Meta Superintelligence Labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa206ea9",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's check our GPU availability and set up the environment for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and CUDA version\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_gpu():\n",
    "    \"\"\"Check if GPU is available and print details.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(\"GPU Information:\")\n",
    "        print(result.stdout)\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(\"No NVIDIA GPU detected. Please enable GPU in Colab:\")\n",
    "        print(\"  Runtime -> Change runtime type -> Hardware accelerator -> GPU\")\n",
    "        return False\n",
    "\n",
    "# Check GPU\n",
    "has_gpu = check_gpu()\n",
    "\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"\\n✓ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"\\n✓ Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ebc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "# Note: This cell may take a few minutes on first run\n",
    "%pip uninstall -y torch torchvision torchaudio\n",
    "%pip install --force-reinstall --no-deps torch==2.9.0+cu126 torchvision==0.24.0+cu126 torchaudio==2.9.0+cu126 --index-url https://download.pytorch.org/whl/cu126\n",
    "%pip install -q transformers==4.43.3 accelerate sentencepiece sacrebleu \"numpy>=2.0,<2.2\" \"faiss-gpu-cu12[fix-cuda]\" \"ipywidgets==7.7.1\"\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "\tprint(f\"CUDA version: {torch.version.cuda}\")\n",
    "\tprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\tprint(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3695f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "HF_TOKEN=userdata.get('HF_TOKEN')\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(HF_TOKEN)\n",
    "    print(\"Successfully logged in to Hugging Face!\")\n",
    "else:\n",
    "    print(\"Token is not set. Please save the token first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e815e7",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Core Utilities\n",
    "\n",
    "Import all necessary libraries and define core utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074dfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "    print(\"✓ FAISS loaded successfully\")\n",
    "except ImportError:\n",
    "    FAISS_AVAILABLE = False\n",
    "    print(\"⚠ FAISS not available - will use CPU fallback\")\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "\n",
    "def seed_everything(seed: int = 1337):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device (GPU/MPS/CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set seeds\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551f79a",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Define all hyperparameters and model configurations. Adjust these based on your GPU memory and training requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d41517",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class REFRAGConfig:\n",
    "    \"\"\"Configuration for REFRAG model and training.\"\"\"\n",
    "    \n",
    "    # Model specifications\n",
    "    encoder_name: str = \"roberta-base\"              # Encoder model for chunk embeddings\n",
    "    decoder_name: str = \"meta-llama/Llama-3.2-1B\"   # Decoder LLM (use smaller for Colab T4)\n",
    "    embed_model: str = \"BAAI/bge-small-en-v1.5\"    # Embedding model for retrieval\n",
    "    \n",
    "    # Compression parameters\n",
    "    chunk_len_tokens: int = 32                      # k: tokens per chunk (compression ratio)\n",
    "    max_q_tokens: int = 256                         # Max question tokens\n",
    "    max_ctx_tokens: int = 1024                      # Max context tokens\n",
    "    max_out_tokens: int = 128                       # Max output tokens\n",
    "    selective_p: float = 0.25                       # Max expansion fraction\n",
    "    \n",
    "    # Training parameters\n",
    "    policy_hidden: int = 256                        # Policy network hidden size\n",
    "    lr: float = 2e-5                                # Learning rate\n",
    "    wd: float = 0.0                                 # Weight decay\n",
    "    grad_clip: float = 1.0                          # Gradient clipping\n",
    "    fp16: bool = False                              # Use mixed precision (disabled for f32)\n",
    "    seed: int = 1337\n",
    "    \n",
    "    # Training steps (reduce for quick testing)\n",
    "    cpt_recon_steps: int = 200                      # CPT reconstruction steps\n",
    "    cpt_next_steps: int = 200                       # CPT next-para steps\n",
    "    policy_steps: int = 200                         # Policy training steps\n",
    "    \n",
    "    # Retrieval\n",
    "    topk: int = 4                                   # Top-k passages to retrieve\n",
    "    \n",
    "# Create default config\n",
    "config = REFRAGConfig()\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = \"data\"\n",
    "RUNS_DIR = \"runs\"\n",
    "INDEX_DIR = os.path.join(RUNS_DIR, \"index\")\n",
    "CPT_RECON_DIR = os.path.join(RUNS_DIR, \"cpt_recon\")\n",
    "CPT_NEXT_DIR = os.path.join(RUNS_DIR, \"cpt_next\")\n",
    "POLICY_DIR = os.path.join(RUNS_DIR, \"policy\")\n",
    "\n",
    "# Create directories\n",
    "for d in [DATA_DIR, RUNS_DIR, INDEX_DIR, CPT_RECON_DIR, CPT_NEXT_DIR, POLICY_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Encoder: {config.encoder_name}\")\n",
    "print(f\"  Decoder: {config.decoder_name}\")\n",
    "print(f\"  Chunk size (k): {config.chunk_len_tokens} tokens\")\n",
    "print(f\"  Expansion fraction (p): {config.selective_p}\")\n",
    "print(f\"  Training steps: recon={config.cpt_recon_steps}, next={config.cpt_next_steps}, policy={config.policy_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ee816",
   "metadata": {},
   "source": [
    "## 4. Data Setup\n",
    "\n",
    "Upload or clone the training data. The notebook expects:\n",
    "- `data/wiki_lines.txt` or `data/corpus_*.txt` - Corpus for retrieval index\n",
    "- `data/cpt_train.jsonl` - CPT training data\n",
    "- `data/rag_train.jsonl` - RAG training data (QA pairs)\n",
    "- `data/rag_eval_test.jsonl` - Evaluation data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "CORPUS_FILE = \"corpus_large.txt\"\n",
    "\n",
    "# Training data\n",
    "CPT_TRAIN_FILE = \"cpt_train.jsonl\"\n",
    "RAG_TRAIN_FILE = \"rag_train.jsonl\"\n",
    "\n",
    "RAG_EVAL_FILE = \"rag_eval_test.jsonl\"\n",
    "\n",
    "# Build full paths\n",
    "corpus_path = os.path.join(DATA_DIR, CORPUS_FILE)\n",
    "cpt_path = os.path.join(DATA_DIR, CPT_TRAIN_FILE)\n",
    "rag_path = os.path.join(DATA_DIR, RAG_TRAIN_FILE)\n",
    "eval_path = os.path.join(DATA_DIR, RAG_EVAL_FILE)\n",
    "\n",
    "# ============================================================\n",
    "# Verify data files exist\n",
    "# ============================================================\n",
    "\n",
    "def check_data_files():\n",
    "    \"\"\"Check if required data files exist and print info.\"\"\"\n",
    "    files_to_check = [\n",
    "        (\"Corpus\", corpus_path),\n",
    "        (\"CPT Training\", cpt_path),\n",
    "        (\"RAG Training\", rag_path),\n",
    "        (\"RAG Evaluation\", eval_path),\n",
    "    ]\n",
    "    \n",
    "    all_found = True\n",
    "    print(\"Data File Status:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name, path in files_to_check:\n",
    "        if os.path.exists(path):\n",
    "            # Get file size and line count\n",
    "            size = os.path.getsize(path) / 1024  # KB\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                lines = sum(1 for _ in f)\n",
    "            print(f\"  ✓ {name}: {path}\")\n",
    "            print(f\"      Size: {size:.1f} KB, Lines: {lines}\")\n",
    "        else:\n",
    "            print(f\"  ✗ {name}: {path} NOT FOUND\")\n",
    "            if name != \"RAG Evaluation\":  # Eval is optional\n",
    "                all_found = False\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if not all_found:\n",
    "        print(\"\\n⚠️  Some required files are missing!\")\n",
    "        print(\"   Please ensure data files are in the 'data/' directory.\")\n",
    "        print(\"   You can:\")\n",
    "        print(\"   1. Upload files manually using the file browser\")\n",
    "        print(\"   2. Clone the repository (uncomment the git clone cell above)\")\n",
    "        print(\"   3. Mount Google Drive and copy files\")\n",
    "    else:\n",
    "        print(\"\\n✓ All required data files found!\")\n",
    "    \n",
    "    return all_found\n",
    "\n",
    "# Check data files\n",
    "data_ready = check_data_files()\n",
    "\n",
    "# Preview the data\n",
    "if data_ready:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Data Preview:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Preview corpus\n",
    "    print(f\"\\nCorpus ({CORPUS_FILE}) - first 3 lines:\")\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            print(f\"  {line.strip()[:100]}...\")\n",
    "    \n",
    "    # Preview CPT data\n",
    "    print(f\"\\nCPT Training ({CPT_TRAIN_FILE}) - first example:\")\n",
    "    with open(cpt_path, 'r', encoding='utf-8') as f:\n",
    "        first_line = f.readline()\n",
    "        if first_line:\n",
    "            ex = json.loads(first_line)\n",
    "            print(f\"  id: {ex.get('id', 'N/A')}\")\n",
    "            print(f\"  tokens: {ex.get('tokens', '')[:150]}...\")\n",
    "            print(f\"  split: {ex.get('split', {})}\")\n",
    "    \n",
    "    # Preview RAG data\n",
    "    print(f\"\\nRAG Training ({RAG_TRAIN_FILE}) - first 3 examples:\")\n",
    "    with open(rag_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            ex = json.loads(line)\n",
    "            print(f\"  Q: {ex.get('question', 'N/A')[:80]}...\")\n",
    "            print(f\"  A: {ex.get('answers', [])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b70257b",
   "metadata": {},
   "source": [
    "## 5. Retrieval Components (FAISS Index)\n",
    "\n",
    "Build the passage encoder and FAISS index for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassageEncoder(nn.Module):\n",
    "    \"\"\"Passage encoder that returns a fixed vector per passage using CLS pooling.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"BAAI/bge-small-en-v1.5\", device=None):\n",
    "        super().__init__()\n",
    "        self.device = device or DEVICE\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.out_dim = self.encoder.config.hidden_size\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_passages(self, texts: List[str], bs: int = 32) -> np.ndarray:\n",
    "        \"\"\"Encode a list of passages into dense vectors.\"\"\"\n",
    "        self.encoder.eval()\n",
    "        if not texts:\n",
    "            return np.zeros((0, self.out_dim), dtype=np.float32)\n",
    "        vecs = []\n",
    "        for i in range(0, len(texts), bs):\n",
    "            batch = texts[i:i+bs]\n",
    "            toks = self.tokenizer(batch, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(self.device)\n",
    "            out = self.encoder(**toks).last_hidden_state\n",
    "            emb = out[:, 0, :]  # CLS token\n",
    "            emb = F.normalize(emb, dim=-1)\n",
    "            vecs.append(emb.detach().cpu().float().numpy())\n",
    "        return np.concatenate(vecs, axis=0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_query(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Encode a single query.\"\"\"\n",
    "        v = self.encode_passages([text], bs=1)\n",
    "        return v[0] if len(v) else np.zeros((self.out_dim,), dtype=np.float32)\n",
    "\n",
    "\n",
    "def build_faiss_index(embeddings: np.ndarray, index_path: str):\n",
    "    \"\"\"Build and save a FAISS index.\"\"\"\n",
    "    if not FAISS_AVAILABLE:\n",
    "        raise RuntimeError(\"FAISS not available. Install with: pip install faiss-cpu or faiss-gpu\")\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)  # Inner product on normalized vectors ≈ cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    faiss.write_index(index, index_path)\n",
    "    return index\n",
    "\n",
    "\n",
    "def load_faiss_index(index_path: str):\n",
    "    \"\"\"Load a FAISS index from disk.\"\"\"\n",
    "    if not FAISS_AVAILABLE:\n",
    "        raise RuntimeError(\"FAISS not available\")\n",
    "    return faiss.read_index(index_path)\n",
    "\n",
    "\n",
    "def search_index(index, query_vec: np.ndarray, topk: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Search the FAISS index for top-k similar passages.\"\"\"\n",
    "    q = query_vec.astype(np.float32)[None, :]\n",
    "    faiss.normalize_L2(q)\n",
    "    D, I = index.search(q, topk)\n",
    "    return D[0], I[0]\n",
    "\n",
    "\n",
    "def load_index_bundle(index_dir: str):\n",
    "    \"\"\"Load the texts and FAISS index.\"\"\"\n",
    "    texts = np.load(os.path.join(index_dir, \"texts.npy\"), allow_pickle=True).tolist()\n",
    "    index = load_faiss_index(os.path.join(index_dir, \"faiss.index\"))\n",
    "    return texts, index\n",
    "\n",
    "\n",
    "print(\"✓ Retrieval components defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e5c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the FAISS index from our corpus\n",
    "print(\"Building FAISS index...\")\n",
    "\n",
    "# Load corpus\n",
    "with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    passages = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "print(f\"  Loaded {len(passages)} passages\")\n",
    "\n",
    "# Initialize passage encoder\n",
    "passage_encoder = PassageEncoder(config.embed_model, device=DEVICE)\n",
    "print(f\"  Encoder loaded: {config.embed_model}\")\n",
    "\n",
    "# Encode passages\n",
    "embeddings = passage_encoder.encode_passages(passages, bs=32)\n",
    "print(f\"  Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Save texts\n",
    "np.save(os.path.join(INDEX_DIR, \"texts.npy\"), np.array(passages, dtype=object))\n",
    "\n",
    "# Build and save index\n",
    "build_faiss_index(embeddings, os.path.join(INDEX_DIR, \"faiss.index\"))\n",
    "print(f\"✓ Index built and saved to {INDEX_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f2064",
   "metadata": {},
   "source": [
    "## 6. REFRAG Model Architecture\n",
    "\n",
    "The core REFRAG model consists of:\n",
    "- **ChunkEncoder**: Encodes text chunks into fixed-size embeddings\n",
    "- **TokenProjector**: Projects encoder embeddings to decoder token space\n",
    "- **SelectPolicy**: RL policy that decides which chunks to expand\n",
    "- **REFRAG**: Main model that orchestrates compression, selection, and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf326ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkEncoder(nn.Module):\n",
    "    \"\"\"Encoder that returns one vector per text chunk via CLS pooling.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(name, use_fast=True)\n",
    "        self.model = AutoModel.from_pretrained(name)\n",
    "        self.out_dim = self.model.config.hidden_size\n",
    "\n",
    "    def forward(self, texts: List[str], device=None) -> torch.Tensor:\n",
    "        device = device or next(self.model.parameters()).device\n",
    "        if len(texts) == 0:\n",
    "            return torch.zeros((0, self.out_dim), device=device)\n",
    "        toks = self.tokenizer(texts, padding=True, truncation=True, max_length=256, return_tensors=\"pt\").to(device)\n",
    "        h = self.model(**toks).last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        h = F.normalize(h, dim=-1)\n",
    "        return h\n",
    "\n",
    "\n",
    "class TokenProjector(nn.Module):\n",
    "    \"\"\"Projection ϕ: encoder-dim → decoder token-embedding dim.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class SelectPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy π(ci) that outputs expansion probability per chunk.\n",
    "    Input: chunk embedding ci (encoder space) + scalar position (normalized [0,1]).\n",
    "    Output: logits ∈ R (Bernoulli).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim: int, hidden: int = 256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim + 1, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, c: torch.Tensor, pos01: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([c, pos01], dim=-1)\n",
    "        return self.net(x).squeeze(-1)  # [L]\n",
    "\n",
    "\n",
    "print(\"✓ Component classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REFRAG(nn.Module):\n",
    "    \"\"\"\n",
    "    REFRAG model: compress → sense/select → expand → decode\n",
    "    \n",
    "    Builds decoder inputs consisting of:\n",
    "      - Question token embeddings (normal)\n",
    "      - Per-chunk compressed embeddings (projected from encoder) OR full token embeddings (expanded)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg: REFRAGConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.device = DEVICE\n",
    "\n",
    "        # Initialize components\n",
    "        print(f\"Loading encoder: {cfg.encoder_name}...\")\n",
    "        self.encoder = ChunkEncoder(cfg.encoder_name).to(self.device)\n",
    "        \n",
    "        print(f\"Loading decoder: {cfg.decoder_name}...\")\n",
    "        self.decoder_tok = AutoTokenizer.from_pretrained(cfg.decoder_name, use_fast=True)\n",
    "        self.decoder = AutoModelForCausalLM.from_pretrained(\n",
    "            cfg.decoder_name,\n",
    "            torch_dtype=torch.float32,  # Always use float32 for consistency\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        )\n",
    "        if not torch.cuda.is_available():\n",
    "            self.decoder = self.decoder.to(self.device)\n",
    "\n",
    "        self.dec_embed_dim = self.decoder.get_input_embeddings().weight.shape[1]\n",
    "        self.projector = TokenProjector(self.encoder.out_dim, self.dec_embed_dim).to(self.device)\n",
    "        self.policy = SelectPolicy(self.encoder.out_dim, hidden=cfg.policy_hidden).to(self.device)\n",
    "\n",
    "        # Token IDs\n",
    "        self.eos_id = self.decoder_tok.eos_token_id\n",
    "        self.pad_id = self.decoder_tok.pad_token_id or self.decoder_tok.eos_token_id\n",
    "        self.bos_id = self.decoder_tok.bos_token_id or self.decoder_tok.eos_token_id\n",
    "        \n",
    "        print(f\"✓ REFRAG initialized (encoder dim: {self.encoder.out_dim}, decoder dim: {self.dec_embed_dim})\")\n",
    "\n",
    "    def _tokenize(self, text: str, max_len: int) -> Dict[str, torch.Tensor]:\n",
    "        return self.decoder_tok(text, truncation=True, max_length=max_len, padding=False, return_tensors=\"pt\")\n",
    "\n",
    "    def _decoder_token_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get decoder token embeddings (no_grad removed to allow gradient flow during training).\"\"\"\n",
    "        return self.decoder.get_input_embeddings()(input_ids.to(self.device))\n",
    "\n",
    "    def _chunk_text(self, text: str, k_tokens: int) -> Tuple[List[str], List[torch.Tensor]]:\n",
    "        \"\"\"Split text into chunks of k tokens.\"\"\"\n",
    "        toks = self.decoder_tok(text, truncation=True, max_length=self.cfg.max_ctx_tokens, return_tensors=\"pt\")\n",
    "        ids = toks.input_ids[0]  # [S]\n",
    "        id_chunks = [ids[i:i+k_tokens] for i in range(0, ids.size(0), k_tokens)]\n",
    "        str_chunks = [self.decoder_tok.decode(ch, skip_special_tokens=True) for ch in id_chunks]\n",
    "        return str_chunks, id_chunks\n",
    "\n",
    "    def _encode_chunks(self, chunk_strs: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Encode chunks using the encoder.\"\"\"\n",
    "        return self.encoder(chunk_strs, device=self.device)\n",
    "\n",
    "    def _project_chunks(self, c: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Project encoder embeddings to decoder space.\"\"\"\n",
    "        return self.projector(c)\n",
    "\n",
    "    def _select_expand_mask(self, c: torch.Tensor, p_max: float) -> torch.Tensor:\n",
    "        \"\"\"Select which chunks to expand using the policy.\"\"\"\n",
    "        L = c.size(0)\n",
    "        if L == 0:\n",
    "            return torch.zeros(0, dtype=torch.bool, device=self.device)\n",
    "        pos01 = torch.linspace(0, 1, steps=L, device=c.device).unsqueeze(-1)\n",
    "        logits = self.policy(c, pos01)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        sample = torch.bernoulli(probs).bool()\n",
    "        \n",
    "        # Enforce max expansion fraction\n",
    "        if p_max > 0.0:\n",
    "            max_expand = max(1, int(round(p_max * L)))  # Fix: ensure at least 1 to avoid topk(k=0) error\n",
    "            if sample.sum().item() > max_expand:\n",
    "                topk = torch.topk(logits, k=max_expand).indices\n",
    "                mask = torch.zeros_like(sample)\n",
    "                mask[topk] = True\n",
    "                sample = mask.bool()\n",
    "        return sample\n",
    "\n",
    "    def build_decoder_inputs(self, question: str, passages: List[str], k: int, p: float, \n",
    "                            use_policy: bool = True) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"Build decoder input embeddings with compression and selective expansion.\"\"\"\n",
    "        # BOS token\n",
    "        bos_emb = self._decoder_token_embeddings(torch.tensor([[self.bos_id]], device=self.device))\n",
    "\n",
    "        # Instruction prefix\n",
    "        instruction = \"Use the following passages to answer the question. Be concise and accurate.\\n\\nPassages:\\n\"\n",
    "        instr_ids = self._tokenize(instruction, 64).input_ids.to(self.device)\n",
    "        instr_emb = self._decoder_token_embeddings(instr_ids)\n",
    "\n",
    "        # Format passages and chunk\n",
    "        formatted_passages = \"\\n\\n\".join([f\"[{i+1}] {p}\" for i, p in enumerate(passages)])\n",
    "        chunk_strs, chunk_ids = self._chunk_text(formatted_passages, k_tokens=k)\n",
    "        L = len(chunk_strs)\n",
    "\n",
    "        # Encode and project chunks\n",
    "        with torch.no_grad():\n",
    "            c = self._encode_chunks(chunk_strs)\n",
    "            ecnk = self._project_chunks(c)\n",
    "\n",
    "        # Select expansions\n",
    "        expand_mask = self._select_expand_mask(c, p_max=p) if use_policy else torch.zeros(L, dtype=torch.bool, device=self.device)\n",
    "\n",
    "        # Build context embeddings\n",
    "        ctx_embs = []\n",
    "        for i, ids in enumerate(chunk_ids):\n",
    "            if expand_mask[i]:\n",
    "                tok_emb = self._decoder_token_embeddings(ids.unsqueeze(0))\n",
    "                ctx_embs.append(tok_emb.squeeze(0))\n",
    "            else:\n",
    "                ctx_embs.append(ecnk[i].unsqueeze(0))\n",
    "\n",
    "        # Question prompt\n",
    "        q_prompt = f\"\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        q_ids = self._tokenize(q_prompt, self.cfg.max_q_tokens).input_ids.to(self.device)\n",
    "        q_emb = self._decoder_token_embeddings(q_ids)\n",
    "\n",
    "        # Concatenate all\n",
    "        seq_embs = [bos_emb.squeeze(0), instr_emb.squeeze(0)] + ctx_embs + [q_emb.squeeze(0)]\n",
    "        final = torch.cat(seq_embs, dim=0).unsqueeze(0)\n",
    "\n",
    "        extras = {\n",
    "            \"expand_mask\": expand_mask.detach().cpu().numpy().tolist(),\n",
    "            \"num_chunks\": L,\n",
    "            \"num_expanded\": expand_mask.sum().item() if L > 0 else 0,\n",
    "        }\n",
    "        return final, extras\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, question: str, passages: List[str], k: int, p: float,\n",
    "                 max_new_tokens: int = 128, temperature: float = 0.0,\n",
    "                 use_policy: bool = True) -> Dict:\n",
    "        \"\"\"Generate answer with REFRAG compression/expansion.\"\"\"\n",
    "        self.decoder.eval()\n",
    "        \n",
    "        # For high p, use standard RAG\n",
    "        if p >= 0.99:\n",
    "            return self._generate_rag_style(question, passages, max_new_tokens, temperature)\n",
    "\n",
    "        emb_in, extras = self.build_decoder_inputs(question, passages, k=k, p=p, use_policy=use_policy)\n",
    "\n",
    "        # Prefill\n",
    "        t0 = time.time()\n",
    "        out = self.decoder(inputs_embeds=emb_in, use_cache=True)\n",
    "        past_key_values = out.past_key_values\n",
    "        ttft = time.time() - t0\n",
    "\n",
    "        # Generate tokens\n",
    "        generated = []\n",
    "        ttit_list = []\n",
    "        logits = out.logits[:, -1, :]\n",
    "        \n",
    "        if temperature > 0.0:\n",
    "            probs = F.softmax(logits / max(temperature, 1e-6), dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        nid = next_id.item()\n",
    "        if nid != self.eos_id:\n",
    "            generated.append(nid)\n",
    "\n",
    "        for _ in range(max_new_tokens - 1):\n",
    "            if nid == self.eos_id:\n",
    "                break\n",
    "            step_emb = self.decoder.get_input_embeddings()(next_id)\n",
    "            t1 = time.time()\n",
    "            out = self.decoder(inputs_embeds=step_emb, use_cache=True, past_key_values=past_key_values)\n",
    "            ttit_list.append(time.time() - t1)\n",
    "\n",
    "            logits = out.logits[:, -1, :]\n",
    "            past_key_values = out.past_key_values\n",
    "            \n",
    "            if temperature > 0.0:\n",
    "                probs = F.softmax(logits / max(temperature, 1e-6), dim=-1)\n",
    "                next_id = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "            nid = next_id.item()\n",
    "            if nid == self.eos_id:\n",
    "                break\n",
    "            generated.append(nid)\n",
    "\n",
    "        text = self.decoder_tok.decode(generated, skip_special_tokens=True)\n",
    "        throughput = (len(generated) / max(sum(ttit_list), 1e-6)) if ttit_list else 0.0\n",
    "        \n",
    "        return {\n",
    "            \"answer\": text.strip(),\n",
    "            \"TTFT_sec\": ttft,\n",
    "            \"TTIT_avg_sec\": float(np.mean(ttit_list)) if ttit_list else 0.0,\n",
    "            \"throughput_tok_per_sec\": throughput,\n",
    "            \"meta\": extras,\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _generate_rag_style(self, question: str, passages: List[str],\n",
    "                            max_new_tokens: int = 128, temperature: float = 0.0) -> Dict:\n",
    "        \"\"\"Standard RAG-style generation (no compression).\"\"\"\n",
    "        context = \"\\n\\n\".join([f\"[{i+1}] {p}\" for i, p in enumerate(passages)])\n",
    "        prompt = f\"\"\"Use the following passages to answer the question. Be concise and accurate.\n",
    "\n",
    "Passages:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        inputs = self.decoder_tok(prompt, return_tensors=\"pt\", truncation=True, \n",
    "                                   max_length=self.cfg.max_ctx_tokens).to(self.device)\n",
    "        input_len = inputs.input_ids.shape[1]\n",
    "\n",
    "        t0 = time.time()\n",
    "        gen_kwargs = {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"pad_token_id\": self.pad_id,\n",
    "            \"eos_token_id\": self.eos_id,\n",
    "            \"do_sample\": temperature > 0,\n",
    "        }\n",
    "        if temperature > 0:\n",
    "            gen_kwargs[\"temperature\"] = temperature\n",
    "\n",
    "        outputs = self.decoder.generate(inputs.input_ids, **gen_kwargs)\n",
    "        ttft = time.time() - t0\n",
    "\n",
    "        generated_ids = outputs[0][input_len:]\n",
    "        text = self.decoder_tok.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        return {\n",
    "            \"answer\": text,\n",
    "            \"TTFT_sec\": ttft,\n",
    "            \"TTIT_avg_sec\": ttft / max(len(generated_ids), 1),\n",
    "            \"throughput_tok_per_sec\": len(generated_ids) / max(ttft, 1e-6),\n",
    "            \"meta\": {\"mode\": \"rag_style\", \"num_chunks\": len(passages)},\n",
    "        }\n",
    "\n",
    "print(\"✓ REFRAG class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e897f4f5",
   "metadata": {},
   "source": [
    "## 7. Training Loss Functions\n",
    "\n",
    "Define the loss functions for the three training phases:\n",
    "1. **Reconstruction Loss**: Train encoder/projector to reconstruct chunk tokens\n",
    "2. **Next-Paragraph Loss**: Train to predict continuation from compressed context\n",
    "3. **Policy Step**: REINFORCE-based reward for selective expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec13ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_reconstruction(model: REFRAG, ctx_text: str, k: int, num_chunks_cap: Optional[int] = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Train encoder+projector to reconstruct tokens chunk-by-chunk.\n",
    "    For each chunk, build context from compressed embeddings and predict tokens.\n",
    "    \"\"\"\n",
    "    chunk_strs, chunk_ids = model._chunk_text(ctx_text, k_tokens=k)\n",
    "    if num_chunks_cap is not None:\n",
    "        chunk_strs = chunk_strs[:num_chunks_cap]\n",
    "        chunk_ids = chunk_ids[:num_chunks_cap]\n",
    "    L = len(chunk_strs)\n",
    "    \n",
    "    if L == 0:\n",
    "        return torch.tensor(0.0, device=model.device, requires_grad=True)\n",
    "\n",
    "    # Encode and project chunks\n",
    "    c = model._encode_chunks(chunk_strs)\n",
    "    e = model._project_chunks(c)\n",
    "\n",
    "    # Per-chunk reconstruction loss\n",
    "    loss_accum = 0.0\n",
    "    for i, ids in enumerate(chunk_ids):\n",
    "        # Build context from previous chunk embeddings\n",
    "        if i == 0:\n",
    "            ctx_emb = e[0].unsqueeze(0).unsqueeze(0)\n",
    "        else:\n",
    "            ctx_emb = e[:i+1].unsqueeze(0)\n",
    "\n",
    "        # Get token embeddings for reconstruction (detach to avoid double backward)\n",
    "        chunk_token_ids = ids.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            chunk_token_embs = model._decoder_token_embeddings(chunk_token_ids.unsqueeze(0))\n",
    "\n",
    "        # Concatenate context and tokens\n",
    "        full_emb = torch.cat([ctx_emb, chunk_token_embs], dim=1)\n",
    "\n",
    "        # Labels: -100 for context, actual tokens for reconstruction\n",
    "        ctx_len = ctx_emb.size(1)\n",
    "        T = chunk_token_ids.size(0)\n",
    "        labels = torch.full((1, ctx_len + T), -100, dtype=torch.long, device=model.device)\n",
    "        labels[0, ctx_len:] = chunk_token_ids\n",
    "\n",
    "        out = model.decoder(inputs_embeds=full_emb, labels=labels)\n",
    "        loss_accum = loss_accum + out.loss\n",
    "\n",
    "    return loss_accum / max(L, 1)\n",
    "\n",
    "\n",
    "def loss_next_para(model: REFRAG, full_text: str, s: int, o: int, k: int, \n",
    "                   expand_frac: float = 0.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Feed first s tokens (compressed) and predict next o tokens.\n",
    "    \"\"\"\n",
    "    toks = model.decoder_tok(full_text, truncation=True, max_length=s + o, return_tensors=\"pt\")\n",
    "    ids = toks.input_ids[0].to(model.device)\n",
    "    total_len = ids.size(0)\n",
    "\n",
    "    min_ctx = max(2 * k, 32)\n",
    "    min_out = max(k, 16)\n",
    "\n",
    "    if total_len < min_ctx + min_out:\n",
    "        return torch.tensor(0.0, device=model.device, requires_grad=True)\n",
    "\n",
    "    # Adaptive split\n",
    "    if total_len >= s + 2:\n",
    "        ctx_len = s\n",
    "        out_len = min(o, total_len - s)\n",
    "    else:\n",
    "        ctx_len = int(total_len * 0.8)\n",
    "        out_len = total_len - ctx_len\n",
    "        if out_len < min_out:\n",
    "            out_len = min_out\n",
    "            ctx_len = total_len - out_len\n",
    "\n",
    "    ctx_ids = ids[:ctx_len]\n",
    "    out_ids = ids[ctx_len:ctx_len + out_len]\n",
    "    ctx_str = model.decoder_tok.decode(ctx_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Chunk and encode context\n",
    "    chunk_strs, chunk_ids = model._chunk_text(ctx_str, k_tokens=k)\n",
    "    c = model._encode_chunks(chunk_strs)\n",
    "    e = model._project_chunks(c)\n",
    "\n",
    "    L = len(chunk_ids)\n",
    "    expand_mask = torch.zeros(L, dtype=torch.bool, device=model.device)\n",
    "    if L > 0 and expand_frac > 0.0:\n",
    "        top = max(1, int(round(expand_frac * L)))\n",
    "        lengths = torch.tensor([len(ch) for ch in chunk_ids], device=model.device)\n",
    "        top_idx = torch.topk(lengths, k=min(top, L)).indices\n",
    "        expand_mask[top_idx] = True\n",
    "\n",
    "    # Build context sequence\n",
    "    seq = []\n",
    "    for i, ids_i in enumerate(chunk_ids):\n",
    "        if expand_mask[i]:\n",
    "            with torch.no_grad():\n",
    "                seq.append(model._decoder_token_embeddings(ids_i.unsqueeze(0)).squeeze(0))\n",
    "        else:\n",
    "            seq.append(e[i].unsqueeze(0))\n",
    "    \n",
    "    if len(seq) == 0:\n",
    "        with torch.no_grad():\n",
    "            seq.append(model._decoder_token_embeddings(ctx_ids.unsqueeze(0)).squeeze(0))\n",
    "\n",
    "    # Concatenate with output tokens (detach output embeddings)\n",
    "    with torch.no_grad():\n",
    "        out_embs = model._decoder_token_embeddings(out_ids.unsqueeze(0)).squeeze(0)\n",
    "    ctx_embs = torch.cat(seq, dim=0)\n",
    "    full_embs = torch.cat([ctx_embs, out_embs], dim=0)\n",
    "    inp = full_embs.unsqueeze(0)\n",
    "\n",
    "    # Labels\n",
    "    ctx_labels = torch.full((ctx_embs.size(0),), -100, dtype=torch.long, device=model.device)\n",
    "    full_labels = torch.cat([ctx_labels, out_ids], dim=0).unsqueeze(0)\n",
    "\n",
    "    out = model.decoder(inputs_embeds=inp, labels=full_labels)\n",
    "    return out.loss\n",
    "\n",
    "\n",
    "def policy_step(model: REFRAG, question: str, passages: List[str], k: int, \n",
    "                max_expand_frac: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    REINFORCE step: sample expansion mask, compute reward = -PPL.\n",
    "    \"\"\"\n",
    "    ctx_text = \"\\n\".join(passages)\n",
    "    chunk_strs, chunk_ids = model._chunk_text(ctx_text, k_tokens=k)\n",
    "    \n",
    "    if len(chunk_strs) == 0:\n",
    "        return torch.tensor(0.0, device=model.device), torch.tensor(0.0, device=model.device)\n",
    "\n",
    "    # Build compressed/expanded context\n",
    "    with torch.no_grad():\n",
    "        c = model._encode_chunks(chunk_strs)\n",
    "    \n",
    "    L = c.size(0)\n",
    "    pos01 = torch.linspace(0, 1, steps=L, device=model.device).unsqueeze(-1)\n",
    "    logits = model.policy(c, pos01)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    bern = torch.distributions.Bernoulli(probs=probs)\n",
    "    sample = bern.sample()\n",
    "\n",
    "    # Enforce max expansion\n",
    "    max_expand = max(1, int(round(max_expand_frac * L)))\n",
    "    mask_modified = False\n",
    "    if sample.sum().item() > max_expand:\n",
    "        top_idx = torch.topk(logits, k=max_expand).indices\n",
    "        mask = torch.zeros_like(sample)\n",
    "        mask[top_idx] = 1.0\n",
    "        sample = mask\n",
    "        mask_modified = True\n",
    "    \n",
    "    # Compute log_prob using the final sample (important for correct gradients)\n",
    "    if mask_modified:\n",
    "        # When mask is modified, use importance-weighted log_prob or just the modified sample\n",
    "        log_prob = bern.log_prob(sample).sum()\n",
    "    else:\n",
    "        log_prob = bern.log_prob(sample).sum()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        e = model._project_chunks(c)\n",
    "    \n",
    "    seq = []\n",
    "    for i, ids_i in enumerate(chunk_ids):\n",
    "        if sample[i] > 0.5:\n",
    "            seq.append(model._decoder_token_embeddings(ids_i.unsqueeze(0)).squeeze(0))\n",
    "        else:\n",
    "            seq.append(e[i].unsqueeze(0))\n",
    "    ctx_emb = torch.cat(seq, dim=0).unsqueeze(0)\n",
    "\n",
    "    # Prepend question\n",
    "    q_ids = model._tokenize(question, model.cfg.max_q_tokens).input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        q_emb = model._decoder_token_embeddings(q_ids)\n",
    "    dec_in = torch.cat([q_emb, ctx_emb], dim=1)\n",
    "\n",
    "    # Quick rollout for target\n",
    "    with torch.no_grad():\n",
    "        out = model.decoder(inputs_embeds=dec_in, use_cache=True)\n",
    "        past = out.past_key_values\n",
    "        rollout = []\n",
    "        last = torch.tensor([[model.eos_id]], device=model.device)\n",
    "        for _ in range(32):\n",
    "            step_emb = model.decoder.get_input_embeddings()(last)\n",
    "            o2 = model.decoder(inputs_embeds=step_emb, use_cache=True, past_key_values=past)\n",
    "            last = torch.argmax(o2.logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            nid = last.item()\n",
    "            if nid == model.eos_id:\n",
    "                break\n",
    "            rollout.append(nid)\n",
    "            past = o2.past_key_values\n",
    "        target = torch.tensor([rollout[:16] or [model.eos_id]], device=model.device, dtype=torch.long)\n",
    "\n",
    "    # Compute reward = -PPL\n",
    "    with torch.no_grad():\n",
    "        tgt_emb = model._decoder_token_embeddings(target)\n",
    "        inputs = torch.cat([dec_in, tgt_emb], dim=1)\n",
    "        labels = torch.full((1, inputs.size(1)), -100, dtype=torch.long, device=model.device)\n",
    "        labels[0, dec_in.size(1):dec_in.size(1) + target.size(1)] = target[0]\n",
    "        out2 = model.decoder(inputs_embeds=inputs, labels=labels)\n",
    "        ppl = torch.exp(out2.loss.detach())\n",
    "\n",
    "    reward = -ppl\n",
    "    return log_prob, reward\n",
    "\n",
    "\n",
    "print(\"✓ Loss functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811a4e5",
   "metadata": {},
   "source": [
    "## 8. Training Utilities\n",
    "\n",
    "Helper functions for optimization and curriculum scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_optim(params, lr: float, wd: float, total_steps: int):\n",
    "    \"\"\"Setup optimizer and learning rate scheduler.\"\"\"\n",
    "    opt = torch.optim.AdamW(params, lr=lr, weight_decay=wd)\n",
    "    sch = get_linear_schedule_with_warmup(\n",
    "        opt, \n",
    "        num_warmup_steps=int(0.06 * total_steps), \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    return opt, sch\n",
    "\n",
    "\n",
    "def curriculum_schedule(total_steps: int, max_chunks: int) -> List[int]:\n",
    "    \"\"\"Linear curriculum: gradually increase chunk count from 1 to max.\"\"\"\n",
    "    plan = []\n",
    "    for t in range(total_steps):\n",
    "        c = 1 + int((max_chunks - 1) * (t / max(1, total_steps - 1)))\n",
    "        plan.append(c)\n",
    "    return plan\n",
    "\n",
    "\n",
    "def load_jsonl(path: str):\n",
    "    \"\"\"Load JSONL file.\"\"\"\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            if ln.strip():\n",
    "                data.append(json.loads(ln))\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"✓ Training utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55987236",
   "metadata": {},
   "source": [
    "## 9. Initialize REFRAG Model\n",
    "\n",
    "Create the REFRAG model instance. This will download and load the encoder and decoder models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize REFRAG model\n",
    "# Note: First run will download model weights (~2-5GB depending on decoder model)\n",
    "\n",
    "print(\"Initializing REFRAG model...\")\n",
    "print(f\"This may take a few minutes on first run (downloading model weights)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "model = REFRAG(config)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Clear cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"  GPU Memory Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7cb74",
   "metadata": {},
   "source": [
    "## 10. Phase A: CPT Reconstruction Training\n",
    "\n",
    "Train the encoder and projector to reconstruct tokens from compressed embeddings.\n",
    "The decoder is frozen during this phase.\n",
    "\n",
    "**Curriculum Learning**: Gradually increase the number of chunks to reconstruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfdf33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cpt_reconstruction(model: REFRAG, data_path: str, steps: int, lr: float,\n",
    "                             k: int, log_every: int = 20, out_dir: str = CPT_RECON_DIR):\n",
    "    \"\"\"\n",
    "    Phase A: Reconstruction curriculum training.\n",
    "    Trains encoder + projector while freezing decoder.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PHASE A: CPT Reconstruction Training\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Freeze decoder\n",
    "    for p in model.decoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Trainable params: encoder + projector\n",
    "    params = list(model.encoder.parameters()) + list(model.projector.parameters())\n",
    "    opt, sch = setup_optim(params, lr=lr, wd=0.0, total_steps=steps)\n",
    "    \n",
    "    # Load data\n",
    "    data = load_jsonl(data_path)\n",
    "    print(f\"Loaded {len(data)} training examples\")\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print(\"No data found!\")\n",
    "        return\n",
    "    \n",
    "    # Estimate max chunks for curriculum\n",
    "    sample_text = data[0][\"tokens\"]\n",
    "    chunk_strs, _ = model._chunk_text(sample_text, k_tokens=k)\n",
    "    max_chunks = max(1, len(chunk_strs))\n",
    "    curriculum = curriculum_schedule(steps, max_chunks)\n",
    "    print(f\"Curriculum: 1 → {max_chunks} chunks over {steps} steps\")\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    t_start = time.time()\n",
    "    for step in range(steps):\n",
    "        ex = random.choice(data)\n",
    "        text = ex[\"tokens\"]\n",
    "        cap = curriculum[step]\n",
    "        \n",
    "        loss = loss_reconstruction(model, text, k=k, num_chunks_cap=cap)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(params, config.grad_clip)\n",
    "        opt.step()\n",
    "        sch.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % log_every == 0:\n",
    "            avg_loss = np.mean(losses[-log_every:]) if len(losses) >= log_every else np.mean(losses)\n",
    "            elapsed = time.time() - t_start\n",
    "            print(f\"  Step {step:4d}/{steps} | Loss: {loss.item():.4f} | Avg: {avg_loss:.4f} | Chunks: {cap} | Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Save\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    torch.save(model.encoder.state_dict(), os.path.join(out_dir, \"encoder.pt\"))\n",
    "    torch.save(model.projector.state_dict(), os.path.join(out_dir, \"projector.pt\"))\n",
    "    print(f\"\\n✓ Saved encoder/projector to {out_dir}\")\n",
    "    print(f\"  Final loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Run reconstruction training\n",
    "recon_losses = train_cpt_reconstruction(\n",
    "    model=model,\n",
    "    data_path=cpt_path,\n",
    "    steps=config.cpt_recon_steps,\n",
    "    lr=config.lr,\n",
    "    k=config.chunk_len_tokens,\n",
    "    log_every=20,\n",
    "    out_dir=CPT_RECON_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd35cb8",
   "metadata": {},
   "source": [
    "## 11. Phase B: CPT Next-Paragraph Training\n",
    "\n",
    "Train the full model (encoder + projector + decoder) on next-paragraph prediction.\n",
    "Uses compressed context to predict continuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f42a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cpt_next_para(model: REFRAG, data_path: str, steps: int, lr: float,\n",
    "                        k: int, expand_frac: float = 0.25, log_every: int = 20,\n",
    "                        load_dir: str = CPT_RECON_DIR, out_dir: str = CPT_NEXT_DIR):\n",
    "    \"\"\"\n",
    "    Phase B: Next-paragraph prediction training.\n",
    "    Trains full model (encoder + projector + decoder unfrozen).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PHASE B: CPT Next-Paragraph Training\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load from reconstruction phase\n",
    "    enc_path = os.path.join(load_dir, \"encoder.pt\")\n",
    "    proj_path = os.path.join(load_dir, \"projector.pt\")\n",
    "    \n",
    "    if os.path.exists(enc_path):\n",
    "        model.encoder.load_state_dict(torch.load(enc_path, map_location=DEVICE))\n",
    "        print(f\"Loaded encoder from {enc_path}\")\n",
    "    if os.path.exists(proj_path):\n",
    "        model.projector.load_state_dict(torch.load(proj_path, map_location=DEVICE))\n",
    "        print(f\"Loaded projector from {proj_path}\")\n",
    "    \n",
    "    # Unfreeze all parameters\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "    \n",
    "    params = list(model.parameters())\n",
    "    opt, sch = setup_optim(params, lr=lr, wd=0.0, total_steps=steps)\n",
    "    \n",
    "    # Load data\n",
    "    data = load_jsonl(data_path)\n",
    "    print(f\"Loaded {len(data)} training examples\")\n",
    "    print(f\"Expansion fraction: {expand_frac}\")\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print(\"No data found!\")\n",
    "        return\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    t_start = time.time()\n",
    "    for step in range(steps):\n",
    "        ex = random.choice(data)\n",
    "        text = ex[\"tokens\"]\n",
    "        s_config = ex.get(\"split\", {}).get(\"s\", 512)\n",
    "        o_config = ex.get(\"split\", {}).get(\"o\", 128)\n",
    "        \n",
    "        loss = loss_next_para(model, text, s=s_config, o=o_config, k=k, expand_frac=expand_frac)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(params, config.grad_clip)\n",
    "        opt.step()\n",
    "        sch.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % log_every == 0:\n",
    "            avg_loss = np.mean(losses[-log_every:]) if len(losses) >= log_every else np.mean(losses)\n",
    "            elapsed = time.time() - t_start\n",
    "            print(f\"  Step {step:4d}/{steps} | Loss: {loss.item():.4f} | Avg: {avg_loss:.4f} | Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Save full model\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(out_dir, \"refrag_full.pt\"))\n",
    "    print(f\"\\n✓ Saved full model to {out_dir}\")\n",
    "    print(f\"  Final loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Run next-paragraph training\n",
    "next_losses = train_cpt_next_para(\n",
    "    model=model,\n",
    "    data_path=cpt_path,\n",
    "    steps=config.cpt_next_steps,\n",
    "    lr=config.lr,\n",
    "    k=config.chunk_len_tokens,\n",
    "    expand_frac=config.selective_p,\n",
    "    log_every=20,\n",
    "    load_dir=CPT_RECON_DIR,\n",
    "    out_dir=CPT_NEXT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab12e99",
   "metadata": {},
   "source": [
    "## 12. Phase C: Policy Training (REINFORCE)\n",
    "\n",
    "Train the selective expansion policy using REINFORCE.\n",
    "The policy learns which chunks to expand for better generation quality.\n",
    "\n",
    "**Reward**: Negative perplexity (-PPL) of the generated continuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68467a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(model: REFRAG, rag_data_path: str, index_dir: str, \n",
    "                 steps: int, lr: float, k: int, p: float, topk: int,\n",
    "                 log_every: int = 20, out_dir: str = POLICY_DIR):\n",
    "    \"\"\"\n",
    "    Train the selective expansion policy using REINFORCE.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PHASE C: Policy Training (REINFORCE)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Freeze everything except policy\n",
    "    for p_param in model.decoder.parameters():\n",
    "        p_param.requires_grad = False\n",
    "    for p_param in model.encoder.parameters():\n",
    "        p_param.requires_grad = False\n",
    "    for p_param in model.projector.parameters():\n",
    "        p_param.requires_grad = False\n",
    "    \n",
    "    # Only train policy\n",
    "    params = list(model.policy.parameters())\n",
    "    opt, sch = setup_optim(params, lr=lr, wd=0.0, total_steps=steps)\n",
    "    \n",
    "    # Load index and data\n",
    "    texts, index = load_index_bundle(index_dir)\n",
    "    data = load_jsonl(rag_data_path)\n",
    "    \n",
    "    print(f\"Index: {len(texts)} passages\")\n",
    "    print(f\"Training data: {len(data)} examples\")\n",
    "    print(f\"Top-k retrieval: {topk}\")\n",
    "    print(f\"Max expansion fraction: {p}\")\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        print(\"No data found!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize query encoder\n",
    "    qenc = PassageEncoder(config.embed_model, device=DEVICE)\n",
    "    \n",
    "    baseline = None\n",
    "    beta = 0.9  # EMA factor\n",
    "    rewards = []\n",
    "    \n",
    "    model.train()\n",
    "    t_start = time.time()\n",
    "    \n",
    "    for step in range(steps):\n",
    "        ex = random.choice(data)\n",
    "        question = ex[\"question\"]\n",
    "        \n",
    "        # Retrieve passages\n",
    "        qv = qenc.encode_query(question)\n",
    "        _, I = search_index(index, qv, topk)\n",
    "        passages = [texts[i] for i in I]\n",
    "        \n",
    "        # Policy step\n",
    "        log_prob, reward = policy_step(model, question, passages, k=k, max_expand_frac=p)\n",
    "        r = reward.item()\n",
    "        \n",
    "        # Update baseline with EMA\n",
    "        baseline = r if baseline is None else (beta * baseline + (1 - beta) * r)\n",
    "        advantage = r - baseline\n",
    "        \n",
    "        # REINFORCE loss\n",
    "        loss = -(log_prob * advantage)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(params, config.grad_clip)\n",
    "        opt.step()\n",
    "        sch.step()\n",
    "        \n",
    "        rewards.append(r)\n",
    "        \n",
    "        if step % log_every == 0:\n",
    "            avg_reward = np.mean(rewards[-log_every:]) if len(rewards) >= log_every else np.mean(rewards)\n",
    "            elapsed = time.time() - t_start\n",
    "            print(f\"  Step {step:4d}/{steps} | Reward: {r:.4f} | Avg: {avg_reward:.4f} | Baseline: {baseline:.4f} | Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Save policy\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    torch.save(model.policy.state_dict(), os.path.join(out_dir, \"policy.pt\"))\n",
    "    print(f\"\\n✓ Saved policy to {out_dir}\")\n",
    "    print(f\"  Final avg reward: {np.mean(rewards[-20:]):.4f}\")\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Run policy training\n",
    "policy_rewards = train_policy(\n",
    "    model=model,\n",
    "    rag_data_path=rag_path,\n",
    "    index_dir=INDEX_DIR,\n",
    "    steps=config.policy_steps,\n",
    "    lr=1e-4,  # Higher LR for policy\n",
    "    k=config.chunk_len_tokens,\n",
    "    p=config.selective_p,\n",
    "    topk=config.topk,\n",
    "    log_every=20,\n",
    "    out_dir=POLICY_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4be00",
   "metadata": {},
   "source": [
    "## 13. Training Visualization\n",
    "\n",
    "Plot the training curves to monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(recon_losses, next_losses, policy_rewards):\n",
    "    \"\"\"Plot training curves for all phases.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Reconstruction loss\n",
    "    if recon_losses:\n",
    "        axes[0].plot(recon_losses, alpha=0.7)\n",
    "        # Smooth curve\n",
    "        window = min(20, len(recon_losses) // 5 + 1)\n",
    "        if window > 1:\n",
    "            smoothed = np.convolve(recon_losses, np.ones(window)/window, mode='valid')\n",
    "            axes[0].plot(range(window-1, len(recon_losses)), smoothed, 'r-', linewidth=2, label='Smoothed')\n",
    "        axes[0].set_title('Phase A: Reconstruction Loss')\n",
    "        axes[0].set_xlabel('Step')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Next-para loss\n",
    "    if next_losses:\n",
    "        axes[1].plot(next_losses, alpha=0.7)\n",
    "        window = min(20, len(next_losses) // 5 + 1)\n",
    "        if window > 1:\n",
    "            smoothed = np.convolve(next_losses, np.ones(window)/window, mode='valid')\n",
    "            axes[1].plot(range(window-1, len(next_losses)), smoothed, 'r-', linewidth=2, label='Smoothed')\n",
    "        axes[1].set_title('Phase B: Next-Para Loss')\n",
    "        axes[1].set_xlabel('Step')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Policy rewards\n",
    "    if policy_rewards:\n",
    "        axes[2].plot(policy_rewards, alpha=0.7)\n",
    "        window = min(20, len(policy_rewards) // 5 + 1)\n",
    "        if window > 1:\n",
    "            smoothed = np.convolve(policy_rewards, np.ones(window)/window, mode='valid')\n",
    "            axes[2].plot(range(window-1, len(policy_rewards)), smoothed, 'r-', linewidth=2, label='Smoothed')\n",
    "        axes[2].set_title('Phase C: Policy Rewards')\n",
    "        axes[2].set_xlabel('Step')\n",
    "        axes[2].set_ylabel('Reward (-PPL)')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training curves\n",
    "plot_training_curves(recon_losses, next_losses, policy_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f1013",
   "metadata": {},
   "source": [
    "## 14. Generation / Inference\n",
    "\n",
    "Test the trained REFRAG model on sample questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5fb658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model: REFRAG, question: str, index_dir: str, topk: int = 4,\n",
    "                  k: int = 32, p: float = 0.25, max_new: int = 128,\n",
    "                  use_policy: bool = True, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Run REFRAG inference on a question.\n",
    "    \"\"\"\n",
    "    # Load index\n",
    "    texts, index = load_index_bundle(index_dir)\n",
    "    \n",
    "    # Encode query and retrieve\n",
    "    qenc = PassageEncoder(config.embed_model, device=DEVICE)\n",
    "    qv = qenc.encode_query(question)\n",
    "    scores, indices = search_index(index, qv, topk)\n",
    "    passages = [texts[i] for i in indices]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nRetrieved Passages (top-{topk}):\")\n",
    "        for i, (p_text, score) in enumerate(zip(passages, scores)):\n",
    "            print(f\"  [{i+1}] (score: {score:.3f}) {p_text[:100]}...\")\n",
    "    \n",
    "    # Generate with REFRAG\n",
    "    result = model.generate(\n",
    "        question=question,\n",
    "        passages=passages,\n",
    "        k=k,\n",
    "        p=p,\n",
    "        max_new_tokens=max_new,\n",
    "        temperature=0.0,\n",
    "        use_policy=use_policy\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"REFRAG Answer: {result['answer']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Performance:\")\n",
    "        print(f\"  - TTFT: {result['TTFT_sec']:.3f} sec\")\n",
    "        print(f\"  - TTIT avg: {result['TTIT_avg_sec']*1000:.2f} ms\")\n",
    "        print(f\"  - Throughput: {result['throughput_tok_per_sec']:.1f} tok/s\")\n",
    "        print(f\"  - Chunks: {result['meta'].get('num_chunks', 'N/A')}\")\n",
    "        print(f\"  - Expanded: {result['meta'].get('num_expanded', 'N/A')}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"Who discovered penicillin?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"What did Alan Turing propose?\",\n",
    "    \"How long is the Great Wall of China?\",\n",
    "    \"Who developed the theory of relativity?\",\n",
    "]\n",
    "\n",
    "# Load trained weights\n",
    "try:\n",
    "    # Try to load full model from CPT\n",
    "    full_path = os.path.join(CPT_NEXT_DIR, \"refrag_full.pt\")\n",
    "    if os.path.exists(full_path):\n",
    "        model.load_state_dict(torch.load(full_path, map_location=DEVICE), strict=False)\n",
    "        print(\"✓ Loaded CPT model weights\")\n",
    "    \n",
    "    # Load policy if available\n",
    "    policy_path = os.path.join(POLICY_DIR, \"policy.pt\")\n",
    "    if os.path.exists(policy_path):\n",
    "        model.policy.load_state_dict(torch.load(policy_path, map_location=DEVICE))\n",
    "        print(\"✓ Loaded policy weights\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not load weights: {e}\")\n",
    "\n",
    "# Run inference on test questions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running REFRAG Inference\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for q in test_questions[:3]:  # Run on first 3 questions\n",
    "    result = run_inference(\n",
    "        model=model,\n",
    "        question=q,\n",
    "        index_dir=INDEX_DIR,\n",
    "        topk=config.topk,\n",
    "        k=config.chunk_len_tokens,\n",
    "        p=config.selective_p,\n",
    "        max_new=config.max_out_tokens,\n",
    "        use_policy=True\n",
    "    )\n",
    "    results.append(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46099086",
   "metadata": {},
   "source": [
    "## 15. Compare REFRAG vs Standard RAG\n",
    "\n",
    "Compare the performance of REFRAG (with compression) vs standard RAG (no compression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_refrag_vs_rag(model: REFRAG, question: str, index_dir: str, topk: int = 4):\n",
    "    \"\"\"\n",
    "    Compare REFRAG (compressed) vs standard RAG (full context).\n",
    "    \"\"\"\n",
    "    texts, index = load_index_bundle(index_dir)\n",
    "    qenc = PassageEncoder(config.embed_model, device=DEVICE)\n",
    "    qv = qenc.encode_query(question)\n",
    "    _, indices = search_index(index, qv, topk)\n",
    "    passages = [texts[i] for i in indices]\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # REFRAG (compressed)\n",
    "    refrag_result = model.generate(\n",
    "        question=question,\n",
    "        passages=passages,\n",
    "        k=config.chunk_len_tokens,\n",
    "        p=config.selective_p,\n",
    "        max_new_tokens=config.max_out_tokens,\n",
    "        temperature=0.0,\n",
    "        use_policy=True\n",
    "    )\n",
    "    \n",
    "    # Standard RAG (no compression, p=1.0)\n",
    "    rag_result = model.generate(\n",
    "        question=question,\n",
    "        passages=passages,\n",
    "        k=config.chunk_len_tokens,\n",
    "        p=1.0,  # Full expansion = standard RAG\n",
    "        max_new_tokens=config.max_out_tokens,\n",
    "        temperature=0.0,\n",
    "        use_policy=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📦 REFRAG (p={config.selective_p}):\")\n",
    "    print(f\"   Answer: {refrag_result['answer']}\")\n",
    "    print(f\"   TTFT: {refrag_result['TTFT_sec']:.3f}s | Throughput: {refrag_result['throughput_tok_per_sec']:.1f} tok/s\")\n",
    "    \n",
    "    print(f\"\\n📄 Standard RAG (p=1.0):\")\n",
    "    print(f\"   Answer: {rag_result['answer']}\")\n",
    "    print(f\"   TTFT: {rag_result['TTFT_sec']:.3f}s | Throughput: {rag_result['throughput_tok_per_sec']:.1f} tok/s\")\n",
    "    \n",
    "    # Speedup\n",
    "    if refrag_result['TTFT_sec'] > 0:\n",
    "        speedup = rag_result['TTFT_sec'] / refrag_result['TTFT_sec']\n",
    "        print(f\"\\n⚡ TTFT Speedup: {speedup:.2f}x\")\n",
    "    \n",
    "    return {\n",
    "        \"refrag\": refrag_result,\n",
    "        \"rag\": rag_result\n",
    "    }\n",
    "\n",
    "# Compare on a sample question\n",
    "print(\"=\"*60)\n",
    "print(\"REFRAG vs Standard RAG Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = compare_refrag_vs_rag(\n",
    "    model=model,\n",
    "    question=\"Who discovered penicillin and when was it discovered?\",\n",
    "    index_dir=INDEX_DIR,\n",
    "    topk=config.topk\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f4800",
   "metadata": {},
   "source": [
    "## 16. Interactive Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_QUESTION = \"What is penicillin used for?\"\n",
    "\n",
    "result = run_inference(\n",
    "    model=model,\n",
    "    question=TEST_QUESTION,\n",
    "    index_dir=INDEX_DIR,\n",
    "    topk=config.topk,\n",
    "    k=config.chunk_len_tokens,\n",
    "    p=config.selective_p,\n",
    "    max_new=config.max_out_tokens,\n",
    "    use_policy=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0524b66",
   "metadata": {},
   "source": [
    "## 17. Save and Load Models\n",
    "\n",
    "Save the trained model to Google Drive for persistence, or load a previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f90bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab only)\n",
    "def mount_drive():\n",
    "    \"\"\"Mount Google Drive for persistent storage.\"\"\"\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"✓ Google Drive mounted\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"Not running in Colab - skipping Drive mount\")\n",
    "        return False\n",
    "\n",
    "# mount_drive()\n",
    "\n",
    "def save_to_drive(model: REFRAG, drive_path: str = \"/content/drive/MyDrive/refrag_models\"):\n",
    "    \"\"\"Save all model components to Google Drive.\"\"\"\n",
    "    os.makedirs(drive_path, exist_ok=True)\n",
    "    \n",
    "    # Save encoder, projector, policy\n",
    "    torch.save(model.encoder.state_dict(), os.path.join(drive_path, \"encoder.pt\"))\n",
    "    torch.save(model.projector.state_dict(), os.path.join(drive_path, \"projector.pt\"))\n",
    "    torch.save(model.policy.state_dict(), os.path.join(drive_path, \"policy.pt\"))\n",
    "    \n",
    "    # Save full model state\n",
    "    torch.save(model.state_dict(), os.path.join(drive_path, \"refrag_full.pt\"))\n",
    "    \n",
    "    # Save config\n",
    "    import pickle\n",
    "    with open(os.path.join(drive_path, \"config.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(config, f)\n",
    "    \n",
    "    print(f\"✓ Model saved to {drive_path}\")\n",
    "\n",
    "def load_from_drive(model: REFRAG, drive_path: str = \"/content/drive/MyDrive/refrag_models\"):\n",
    "    \"\"\"Load model components from Google Drive.\"\"\"\n",
    "    try:\n",
    "        full_path = os.path.join(drive_path, \"refrag_full.pt\")\n",
    "        if os.path.exists(full_path):\n",
    "            model.load_state_dict(torch.load(full_path, map_location=DEVICE), strict=False)\n",
    "            print(f\"✓ Loaded full model from {drive_path}\")\n",
    "        else:\n",
    "            # Load individual components\n",
    "            enc_path = os.path.join(drive_path, \"encoder.pt\")\n",
    "            proj_path = os.path.join(drive_path, \"projector.pt\")\n",
    "            pol_path = os.path.join(drive_path, \"policy.pt\")\n",
    "            \n",
    "            if os.path.exists(enc_path):\n",
    "                model.encoder.load_state_dict(torch.load(enc_path, map_location=DEVICE))\n",
    "            if os.path.exists(proj_path):\n",
    "                model.projector.load_state_dict(torch.load(proj_path, map_location=DEVICE))\n",
    "            if os.path.exists(pol_path):\n",
    "                model.policy.load_state_dict(torch.load(pol_path, map_location=DEVICE))\n",
    "            print(f\"✓ Loaded model components from {drive_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "\n",
    "# Uncomment to save/load:\n",
    "# save_to_drive(model)\n",
    "# load_from_drive(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25009f4",
   "metadata": {},
   "source": [
    "## 18. Summary & Next Steps\n",
    "\n",
    "### What We've Done\n",
    "1. ✅ Set up the Colab environment with GPU support\n",
    "2. ✅ Built a FAISS index for passage retrieval\n",
    "3. ✅ Trained REFRAG in three phases:\n",
    "   - **Phase A**: Reconstruction curriculum (encoder + projector)\n",
    "   - **Phase B**: Next-paragraph prediction (full model)\n",
    "   - **Phase C**: Policy training (REINFORCE)\n",
    "4. ✅ Generated answers with REFRAG compression\n",
    "5. ✅ Compared REFRAG vs standard RAG performance\n",
    "\n",
    "### Key Hyperparameters to Tune\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|---------|\n",
    "| `chunk_len_tokens` (k) | Tokens per chunk - lower = more compression | 32 |\n",
    "| `selective_p` | Max expansion fraction | 0.25 |\n",
    "| `cpt_*_steps` | Training steps per phase | 200 |\n",
    "| `lr` | Learning rate | 2e-5 |\n",
    "| `topk` | Number of retrieved passages | 4 |\n",
    "\n",
    "### Next Steps\n",
    "- **Scale up training**: Increase training steps for better convergence\n",
    "- **Use larger decoder**: Try `meta-llama/Llama-3.2-3B` for better quality\n",
    "- **Add more data**: Train on larger corpus for better retrieval\n",
    "- **Experiment with k and p**: Find optimal compression/quality trade-off\n",
    "\n",
    "### Resources\n",
    "- [REFRAG Paper](https://arxiv.org/abs/...) (Meta Superintelligence Labs)\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
    "- [FAISS Documentation](https://faiss.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Free GPU memory if needed\n",
    "def cleanup():\n",
    "    \"\"\"Free GPU memory.\"\"\"\n",
    "    global model, passage_encoder\n",
    "    import gc\n",
    "    \n",
    "    # Delete models\n",
    "    try:\n",
    "        del model\n",
    "        del passage_encoder\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Clear cache\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    \n",
    "    print(\"✓ Cleanup complete\")\n",
    "\n",
    "# Uncomment to free GPU memory:\n",
    "# cleanup()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 REFRAG v1 Training and Inference Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nYou can now:\")\n",
    "print(\"  1. Re-run training cells with different hyperparameters\")\n",
    "print(\"  2. Test with your own questions in section 16\")\n",
    "print(\"  3. Save model to Google Drive for later use\")\n",
    "print(\"  4. Add own corpus data for domain-specific RAG\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
