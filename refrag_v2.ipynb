{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# REFRAG v2 - Training Pipeline for Google Colab\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/khanhtimn/Project-III/blob/main/refrag_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook implements the full REFRAG v2 training pipeline, designed to run on Google Colab with GPU acceleration.\n",
        "\n",
        "## Pipeline Overview\n",
        "\n",
        "1. **Setup**: Install dependencies and configure environment\n",
        "2. **Data Preparation**: Upload or download training data\n",
        "3. **Stage 0**: Build FAISS index from corpus\n",
        "4. **Stage 1**: Reconstruction training (freeze decoder, train encoder + projector)\n",
        "5. **Stage 2**: Continual Pre-Training (CPT) - unfreeze decoder\n",
        "6. **Stage 3**: Policy training (optional, GRPO-style)\n",
        "7. **Evaluation**: Test generation and compare models\n",
        "\n",
        "## REFRAG Paper Summary\n",
        "\n",
        "REFRAG (Retrieval-Enhanced Framework for RAG) compresses retrieved passages into dense embeddings:\n",
        "- Compresses k tokens into 1 embedding (k=16 recommended: 30Ã— speedup, -0.3% accuracy)\n",
        "- Uses RoBERTa-Large encoder + LLaMA decoder\n",
        "- 9-stage curriculum learning for training\n",
        "- Optional selective expansion policy via GRPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V07fQ8EmJeYf",
        "outputId": "568d608f-3b8f-4d86-8eb4-1b9ddf757f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n"
          ]
        }
      ],
      "source": [
        "# @title ðŸ”§ Environment Setup { display-mode: \"form\" }\n",
        "# @markdown Check GPU availability and setup environment\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Check if running on Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"âœ… Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"âš ï¸ Not running in Colab - some features may differ\")\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"âœ… GPU Available: {gpu_name}\")\n",
        "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
        "    DEVICE = \"cuda\"\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    print(\"âœ… MPS (Apple Silicon) Available\")\n",
        "    DEVICE = \"mps\"\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU detected - training will be slow\")\n",
        "    DEVICE = \"cpu\"\n",
        "\n",
        "print(f\"\\nðŸ“ Device: {DEVICE}\")\n",
        "print(f\"ðŸ Python: {sys.version.split()[0]}\")\n",
        "print(f\"ðŸ”¥ PyTorch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ“¦ Install Dependencies { display-mode: \"form\" }\n",
        "# @markdown Install all required packages for REFRAG v2\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install required packages for REFRAG v2.\"\"\"\n",
        "    packages = [\n",
        "        \"torch>=2.0.0\",\n",
        "        \"transformers>=4.40.0\",\n",
        "        \"accelerate>=0.20.0\",\n",
        "        \"sentencepiece>=0.1.99\",\n",
        "        \"safetensors>=0.4.0\",\n",
        "        \"faiss-cpu>=1.7.4\",  # Use faiss-gpu on Colab if available\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"datasets>=2.14.0\",\n",
        "        \"tqdm>=4.65.0\",\n",
        "        \"numpy>=1.24.0\",\n",
        "    ]\n",
        "    \n",
        "    print(\"ðŸ“¦ Installing dependencies...\")\n",
        "    for pkg in packages:\n",
        "        pkg_name = pkg.split(\">=\")[0].split(\"==\")[0]\n",
        "        try:\n",
        "            __import__(pkg_name.replace(\"-\", \"_\"))\n",
        "            print(f\"  âœ… {pkg_name} already installed\")\n",
        "        except ImportError:\n",
        "            print(f\"  ðŸ“¥ Installing {pkg}...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "    \n",
        "    # Try to install faiss-gpu on Colab (falls back to cpu)\n",
        "    try:\n",
        "        import google.colab\n",
        "        print(\"  ðŸ“¥ Attempting faiss-gpu for Colab...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-gpu\"])\n",
        "        print(\"  âœ… faiss-gpu installed\")\n",
        "    except:\n",
        "        print(\"  â„¹ï¸ Using faiss-cpu\")\n",
        "    \n",
        "    print(\"\\nâœ… All dependencies installed!\")\n",
        "\n",
        "install_packages()\n",
        "\n",
        "# Verify key imports\n",
        "import torch\n",
        "import transformers\n",
        "import faiss\n",
        "print(f\"\\nðŸ” Verification:\")\n",
        "print(f\"   torch: {torch.__version__}\")\n",
        "print(f\"   transformers: {transformers.__version__}\")\n",
        "print(f\"   faiss: {faiss.__version__ if hasattr(faiss, '__version__') else 'installed'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”‘ Hugging Face Authentication\n",
        "\n",
        "You need to authenticate with Hugging Face to access gated models like LLaMA. \n",
        "Get your token from: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ” HuggingFace Login { display-mode: \"form\" }\n",
        "# @markdown Enter your HuggingFace token (required for LLaMA models)\n",
        "\n",
        "from huggingface_hub import login, whoami\n",
        "\n",
        "# Check if already logged in\n",
        "try:\n",
        "    user_info = whoami()\n",
        "    print(f\"âœ… Already logged in as: {user_info['name']}\")\n",
        "except:\n",
        "    print(\"ðŸ” Please login to HuggingFace...\")\n",
        "    try:\n",
        "        # For Colab, use notebook_login for interactive widget\n",
        "        from google.colab import userdata\n",
        "        try:\n",
        "            # Try to get token from Colab secrets\n",
        "            hf_token = userdata.get('HF_TOKEN')\n",
        "            login(token=hf_token)\n",
        "            print(\"âœ… Logged in using Colab secret!\")\n",
        "        except:\n",
        "            # Fall back to interactive login\n",
        "            from huggingface_hub import notebook_login\n",
        "            notebook_login()\n",
        "    except ImportError:\n",
        "        # Not in Colab, use standard login\n",
        "        login()\n",
        "    \n",
        "    # Verify login\n",
        "    user_info = whoami()\n",
        "    print(f\"âœ… Logged in as: {user_info['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Configuration\n",
        "\n",
        "Configure the training parameters below. Adjust based on your GPU memory:\n",
        "- **T4 (16GB)**: Use smaller batch size (2-4), k=16, smaller decoder model\n",
        "- **A100 (40GB)**: Can use batch size 8, k=16, full LLaMA-3B\n",
        "- **L4/A10 (24GB)**: Moderate settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title âš™ï¸ Training Configuration { display-mode: \"form\" }\n",
        "# @markdown ### Model Configuration\n",
        "ENCODER_MODEL = \"roberta-large\"  # @param [\"roberta-large\", \"roberta-base\", \"distilroberta-base\"]\n",
        "DECODER_MODEL = \"meta-llama/Llama-3.2-1B\"  # @param [\"meta-llama/Llama-3.2-1B\", \"meta-llama/Llama-3.2-3B\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"microsoft/phi-2\"]\n",
        "EMBED_MODEL = \"BAAI/bge-small-en-v1.5\"  # @param [\"BAAI/bge-small-en-v1.5\", \"sentence-transformers/all-MiniLM-L6-v2\"]\n",
        "\n",
        "# @markdown ### Compression Settings\n",
        "K = 16  # @param {type:\"slider\", min:8, max:64, step:8}\n",
        "# @markdown K is the compression ratio (tokens per embedding). k=16 recommended.\n",
        "\n",
        "# @markdown ### Training Hyperparameters\n",
        "BATCH_SIZE = 4  # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "NUM_CURRICULUM_STAGES = 5  # @param {type:\"slider\", min:1, max:9, step:1}\n",
        "# @markdown Reduce stages for faster training (9 is paper default)\n",
        "\n",
        "LR_RECONSTRUCTION = 2e-4  # @param {type:\"number\"}\n",
        "LR_CPT = 1e-5  # @param {type:\"number\"}\n",
        "LR_CPT_DECODER = 5e-7  # @param {type:\"number\"}\n",
        "\n",
        "# @markdown ### Retrieval Settings  \n",
        "TOP_K = 4  # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "# @markdown ### Advanced Settings\n",
        "USE_FP16 = False  # @param {type:\"boolean\"}\n",
        "# @markdown FP16 can cause NaN issues - disable if training becomes unstable\n",
        "SEED = 1337  # @param {type:\"integer\"}\n",
        "\n",
        "# Print configuration summary\n",
        "print(\"=\" * 50)\n",
        "print(\"ðŸ“‹ REFRAG v2 Configuration\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Encoder:     {ENCODER_MODEL}\")\n",
        "print(f\"Decoder:     {DECODER_MODEL}\")\n",
        "print(f\"Embed Model: {EMBED_MODEL}\")\n",
        "print(f\"K (chunk):   {K}\")\n",
        "print(f\"Batch Size:  {BATCH_SIZE}\")\n",
        "print(f\"Stages:      {NUM_CURRICULUM_STAGES}\")\n",
        "print(f\"FP16:        {USE_FP16}\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“‚ Data Preparation\n",
        "\n",
        "You can either:\n",
        "1. **Upload your own data** - Upload files to `data/` directory\n",
        "2. **Use sample data** - Generate synthetic training data\n",
        "3. **Download from HuggingFace** - Load datasets from the Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ“‚ Setup Data Directories { display-mode: \"form\" }\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Create directory structure\n",
        "DATA_DIR = \"data\"\n",
        "RUNS_DIR = \"runs\"\n",
        "INDEX_DIR = os.path.join(RUNS_DIR, \"index\")\n",
        "RECON_DIR = os.path.join(RUNS_DIR, \"refrag_v2_recon\")\n",
        "CPT_DIR = os.path.join(RUNS_DIR, \"refrag_v2_cpt\")\n",
        "POLICY_DIR = os.path.join(RUNS_DIR, \"refrag_v2_policy\")\n",
        "\n",
        "for d in [DATA_DIR, INDEX_DIR, RECON_DIR, CPT_DIR, POLICY_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "    print(f\"âœ… Created: {d}\")\n",
        "\n",
        "# File paths\n",
        "CORPUS_FILE = os.path.join(DATA_DIR, \"corpus.txt\")\n",
        "CPT_TRAIN_FILE = os.path.join(DATA_DIR, \"cpt_train.jsonl\")\n",
        "RAG_TRAIN_FILE = os.path.join(DATA_DIR, \"rag_train.jsonl\")\n",
        "RAG_EVAL_FILE = os.path.join(DATA_DIR, \"rag_eval.jsonl\")\n",
        "\n",
        "print(f\"\\nðŸ“ Data directory: {os.path.abspath(DATA_DIR)}\")\n",
        "print(f\"ðŸ“ Runs directory: {os.path.abspath(RUNS_DIR)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ“¤ Option 1: Upload Your Data (Colab) { display-mode: \"form\" }\n",
        "# @markdown Run this cell to upload your own data files\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    \n",
        "    print(\"ðŸ“¤ Upload your data files:\")\n",
        "    print(\"   - corpus.txt (one passage per line)\")\n",
        "    print(\"   - cpt_train.jsonl (training data with 'text' field)\")\n",
        "    print(\"   - rag_train.jsonl (QA pairs with 'question' and 'answers' fields)\")\n",
        "    print(\"\\nâ³ Waiting for file upload...\")\n",
        "    \n",
        "    uploaded = files.upload()\n",
        "    \n",
        "    for filename, content in uploaded.items():\n",
        "        filepath = os.path.join(DATA_DIR, filename)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            f.write(content)\n",
        "        print(f\"âœ… Saved: {filepath} ({len(content)} bytes)\")\n",
        "        \n",
        "except ImportError:\n",
        "    print(\"â„¹ï¸ Not running in Colab. Place your data files in the 'data/' directory manually.\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Upload cancelled or failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ”§ Option 2: Generate Sample Data { display-mode: \"form\" }\n",
        "# @markdown Generate synthetic training data for testing the pipeline\n",
        "\n",
        "import random\n",
        "\n",
        "def generate_sample_data(num_passages=500, num_qa=200):\n",
        "    \"\"\"Generate synthetic training data for REFRAG.\"\"\"\n",
        "    \n",
        "    # Generate corpus (passages)\n",
        "    passages = []\n",
        "    for i in range(num_passages):\n",
        "        city_id = i % 100\n",
        "        country_id = i % 50\n",
        "        river_id = i * 2\n",
        "        year = 1200 + (i * 3) % 800\n",
        "        pop = 50000 + i * 500\n",
        "        \n",
        "        passage = f\"TITLE: Atlas Entry â€” City_{city_id}. City_{city_id} is a regional hub in Country_{country_id:03d}. \" \\\n",
        "                  f\"It lies on the banks of River_{river_id:03d} and was founded in {year}. \" \\\n",
        "                  f\"The metropolitan population is approximately {pop}. \" \\\n",
        "                  f\"The primary research institution is University_{i % 200:03d}. \" \\\n",
        "                  f\"City_{city_id} specializes in Industry_{i % 100:03d} and advanced logistics.\"\n",
        "        passages.append(passage)\n",
        "    \n",
        "    # Add some variety - materials and biographies\n",
        "    for i in range(num_passages // 4):\n",
        "        alloy_passage = f\"TITLE: Encyclopedia of Materials â€” Alloy_{i}. \" \\\n",
        "                       f\"Composition: Metal_{i % 100:02d} {60 + i % 20}%, Additive_{i % 50:02d} {30 - i % 10}%. \" \\\n",
        "                       f\"Melting point: {900 + i * 3} Â°C. Yield strengths near {200 + i * 5} MPa.\"\n",
        "        passages.append(alloy_passage)\n",
        "        \n",
        "        bio_passage = f\"TITLE: Biography â€” Person_{i}. Person_{i} is a researcher in Field_{i % 50:02d} \" \\\n",
        "                     f\"affiliated with Institution_{i % 200:03d}. In {1950 + i % 60}, they proposed Contribution_{i:04d}.\"\n",
        "        passages.append(bio_passage)\n",
        "    \n",
        "    # Write corpus\n",
        "    with open(CORPUS_FILE, 'w', encoding='utf-8') as f:\n",
        "        for p in passages:\n",
        "            f.write(p + '\\n')\n",
        "    print(f\"âœ… Generated corpus: {len(passages)} passages -> {CORPUS_FILE}\")\n",
        "    \n",
        "    # Generate CPT training data (longer text for reconstruction)\n",
        "    cpt_data = []\n",
        "    for i in range(min(len(passages), 400)):\n",
        "        # Combine a few passages for longer context\n",
        "        combined = \" \".join(passages[i:i+3])\n",
        "        cpt_data.append({\"id\": f\"cpt-{i}\", \"text\": combined})\n",
        "    \n",
        "    with open(CPT_TRAIN_FILE, 'w', encoding='utf-8') as f:\n",
        "        for item in cpt_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    print(f\"âœ… Generated CPT training data: {len(cpt_data)} examples -> {CPT_TRAIN_FILE}\")\n",
        "    \n",
        "    # Generate RAG QA data\n",
        "    qa_data = []\n",
        "    for i in range(num_qa):\n",
        "        city_id = i % 100\n",
        "        q_type = i % 4\n",
        "        \n",
        "        if q_type == 0:\n",
        "            question = f\"Which river flows through City_{city_id}?\"\n",
        "            answer = f\"River_{city_id * 2:03d}\"\n",
        "        elif q_type == 1:\n",
        "            question = f\"What is the primary research institution in City_{city_id}?\"\n",
        "            answer = f\"University_{city_id % 200:03d}\"\n",
        "        elif q_type == 2:\n",
        "            question = f\"What is the melting point of Alloy_{city_id}?\"\n",
        "            answer = f\"{900 + city_id * 3} Â°C\"\n",
        "        else:\n",
        "            question = f\"In which field does Person_{city_id} work?\"\n",
        "            answer = f\"Field_{city_id % 50:02d}\"\n",
        "        \n",
        "        qa_data.append({\n",
        "            \"id\": f\"qa-{i}\",\n",
        "            \"question\": question,\n",
        "            \"answers\": [answer]\n",
        "        })\n",
        "    \n",
        "    # Split into train and eval\n",
        "    random.shuffle(qa_data)\n",
        "    split_idx = int(len(qa_data) * 0.8)\n",
        "    train_qa = qa_data[:split_idx]\n",
        "    eval_qa = qa_data[split_idx:]\n",
        "    \n",
        "    with open(RAG_TRAIN_FILE, 'w', encoding='utf-8') as f:\n",
        "        for item in train_qa:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    print(f\"âœ… Generated RAG training data: {len(train_qa)} examples -> {RAG_TRAIN_FILE}\")\n",
        "    \n",
        "    with open(RAG_EVAL_FILE, 'w', encoding='utf-8') as f:\n",
        "        for item in eval_qa:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    print(f\"âœ… Generated RAG eval data: {len(eval_qa)} examples -> {RAG_EVAL_FILE}\")\n",
        "    \n",
        "    return len(passages), len(cpt_data), len(train_qa), len(eval_qa)\n",
        "\n",
        "# Generate data\n",
        "n_passages, n_cpt, n_train, n_eval = generate_sample_data()\n",
        "print(f\"\\nðŸ“Š Data Summary:\")\n",
        "print(f\"   Corpus passages: {n_passages}\")\n",
        "print(f\"   CPT examples: {n_cpt}\")\n",
        "print(f\"   RAG train QA: {n_train}\")\n",
        "print(f\"   RAG eval QA: {n_eval}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ—ï¸ REFRAG v2 Model Implementation\n",
        "\n",
        "The following cells contain the complete REFRAG v2 implementation:\n",
        "- Configuration dataclass\n",
        "- Encoder, Projector, and Policy networks\n",
        "- Full REFRAG model with training methods\n",
        "- Training loops for all three stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ—ï¸ REFRAG v2 Core Implementation { display-mode: \"form\" }\n",
        "# @markdown This cell contains the complete REFRAG v2 model implementation\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple, Dict, Optional, Any\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoModelForCausalLM,\n",
        "    get_cosine_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class REFRAGConfig:\n",
        "    \"\"\"Configuration for REFRAG v2 model.\"\"\"\n",
        "    encoder_name: str = ENCODER_MODEL\n",
        "    decoder_name: str = DECODER_MODEL\n",
        "    chunk_size_k: int = K\n",
        "    db_chunk_size: int = 256\n",
        "    max_context_tokens: int = 128\n",
        "    max_output_tokens: int = 128\n",
        "    max_query_tokens: int = 256\n",
        "    expansion_fraction_p: float = 0.1\n",
        "    batch_size: int = BATCH_SIZE\n",
        "    gradient_accumulation_steps: int = 32\n",
        "    lr_reconstruction: float = LR_RECONSTRUCTION\n",
        "    lr_cpt: float = LR_CPT\n",
        "    lr_cpt_decoder: float = LR_CPT_DECODER\n",
        "    lr_finetune: float = 2e-5\n",
        "    lr_policy: float = 1e-4\n",
        "    weight_decay: float = 0.0\n",
        "    warmup_ratio: float = 0.04\n",
        "    max_grad_norm: float = 1.0\n",
        "    num_curriculum_stages: int = NUM_CURRICULUM_STAGES\n",
        "    epochs_per_stage: int = 1\n",
        "    policy_hidden_dim: int = 256\n",
        "    grpo_group_size: int = 4\n",
        "    ppo_clip_epsilon: float = 0.2\n",
        "    fp16: bool = USE_FP16\n",
        "    seed: int = SEED\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        return asdict(self)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Utilities\n",
        "# ============================================================================\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Get the best available device.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        return torch.device('mps')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Curriculum Learning Schedule\n",
        "# ============================================================================\n",
        "\n",
        "def get_curriculum_schedule(num_stages: int = 9, max_chunks: int = 256) -> List[Dict]:\n",
        "    \"\"\"Generate the curriculum schedule from the paper.\"\"\"\n",
        "    chunk_multipliers = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
        "    schedule = []\n",
        "\n",
        "    for stage in range(num_stages):\n",
        "        stage_config = {\n",
        "            'stage': stage + 1,\n",
        "            'max_chunks': min(chunk_multipliers[stage] if stage < len(chunk_multipliers) else max_chunks, max_chunks),\n",
        "            'weights': {}\n",
        "        }\n",
        "\n",
        "        for j, mult in enumerate(chunk_multipliers):\n",
        "            if mult > max_chunks:\n",
        "                continue\n",
        "            if j <= stage:\n",
        "                weight = 0.5 ** (stage - j)\n",
        "            else:\n",
        "                weight = 0.5 ** (j - stage) * 0.1\n",
        "            stage_config['weights'][mult] = weight\n",
        "\n",
        "        total = sum(stage_config['weights'].values())\n",
        "        for k in stage_config['weights']:\n",
        "            stage_config['weights'][k] /= total\n",
        "\n",
        "        schedule.append(stage_config)\n",
        "\n",
        "    return schedule\n",
        "\n",
        "\n",
        "def sample_num_chunks_for_stage(stage_config: Dict) -> int:\n",
        "    \"\"\"Sample number of chunks based on stage weights.\"\"\"\n",
        "    chunks = list(stage_config['weights'].keys())\n",
        "    weights = list(stage_config['weights'].values())\n",
        "    return random.choices(chunks, weights=weights, k=1)[0]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Data Loading\n",
        "# ============================================================================\n",
        "\n",
        "class CPTDataset(Dataset):\n",
        "    \"\"\"Dataset for Continual Pre-Training.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, tokenizer, max_length: int = 4096):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.data = []\n",
        "\n",
        "        if os.path.exists(data_path):\n",
        "            with open(data_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    if line.strip():\n",
        "                        item = json.loads(line)\n",
        "                        if 'text' in item or 'tokens' in item:\n",
        "                            self.data.append(item)\n",
        "        logger.info(f\"Loaded {len(self.data)} examples from {data_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item.get('text', item.get('tokens', ''))\n",
        "        return {'text': text, 'id': item.get('id', str(idx))}\n",
        "\n",
        "\n",
        "class RAGDataset(Dataset):\n",
        "    \"\"\"Dataset for RAG training/evaluation.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str):\n",
        "        self.data = []\n",
        "        if os.path.exists(data_path):\n",
        "            with open(data_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    if line.strip():\n",
        "                        item = json.loads(line)\n",
        "                        if 'question' in item:\n",
        "                            self.data.append(item)\n",
        "        logger.info(f\"Loaded {len(self.data)} QA examples from {data_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "print(\"âœ… Core utilities loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ” Passage Retriever (FAISS) { display-mode: \"form\" }\n",
        "# @markdown FAISS-based passage retriever for building and searching the index\n",
        "\n",
        "import faiss\n",
        "\n",
        "class PassageRetriever:\n",
        "    \"\"\"FAISS-based passage retriever.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = EMBED_MODEL, device=None):\n",
        "        self.device = device or get_device()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        self.model.eval()\n",
        "        self.embed_dim = self.model.config.hidden_size\n",
        "        self.index = None\n",
        "        self.passages = []\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
        "        \"\"\"Encode texts to vectors.\"\"\"\n",
        "        all_embeddings = []\n",
        "\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\", leave=False):\n",
        "            batch = texts[i:i + batch_size]\n",
        "            inputs = self.tokenizer(\n",
        "                batch, padding=True, truncation=True, max_length=512, return_tensors='pt'\n",
        "            ).to(self.device)\n",
        "\n",
        "            outputs = self.model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :]  # CLS pooling\n",
        "            embeddings = F.normalize(embeddings, dim=-1)\n",
        "            all_embeddings.append(embeddings.cpu().numpy())\n",
        "\n",
        "        return np.vstack(all_embeddings).astype(np.float32)\n",
        "\n",
        "    def build_index(self, passages: List[str], index_path: str):\n",
        "        \"\"\"Build and save FAISS index.\"\"\"\n",
        "        self.passages = passages\n",
        "        print(f\"ðŸ“Š Encoding {len(passages)} passages...\")\n",
        "        embeddings = self.encode(passages)\n",
        "\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index = faiss.IndexFlatIP(self.embed_dim)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "        os.makedirs(os.path.dirname(index_path) or '.', exist_ok=True)\n",
        "        faiss.write_index(self.index, index_path)\n",
        "        np.save(index_path.replace('.index', '_passages.npy'), np.array(passages, dtype=object))\n",
        "\n",
        "        print(f\"âœ… Built index with {len(passages)} passages -> {index_path}\")\n",
        "\n",
        "    def load_index(self, index_path: str):\n",
        "        \"\"\"Load FAISS index.\"\"\"\n",
        "        self.index = faiss.read_index(index_path)\n",
        "        passages_path = index_path.replace('.index', '_passages.npy')\n",
        "        self.passages = np.load(passages_path, allow_pickle=True).tolist()\n",
        "        print(f\"âœ… Loaded index with {len(self.passages)} passages\")\n",
        "\n",
        "    def search(self, query: str, top_k: int = 10) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Search for relevant passages.\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not loaded\")\n",
        "\n",
        "        query_vec = self.encode([query])\n",
        "        faiss.normalize_L2(query_vec)\n",
        "        scores, indices = self.index.search(query_vec, top_k)\n",
        "\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx >= 0 and idx < len(self.passages):\n",
        "                results.append((self.passages[idx], float(score)))\n",
        "        return results\n",
        "\n",
        "\n",
        "print(\"âœ… Passage retriever loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ§  REFRAG Model Components { display-mode: \"form\" }\n",
        "# @markdown Encoder, Projector, and Policy network definitions\n",
        "\n",
        "class ChunkEncoder(nn.Module):\n",
        "    \"\"\"Encoder for chunk embeddings using RoBERTa.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"roberta-large\"):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.hidden_size = self.model.config.hidden_size\n",
        "\n",
        "    def forward(self, texts: List[str], device=None) -> torch.Tensor:\n",
        "        \"\"\"Encode text chunks to embeddings. Returns: [num_chunks, hidden_size]\"\"\"\n",
        "        if len(texts) == 0:\n",
        "            return torch.zeros(0, self.hidden_size, device=device)\n",
        "\n",
        "        device = device or next(self.model.parameters()).device\n",
        "        inputs = self.tokenizer(\n",
        "            texts, padding=True, truncation=True, max_length=512, return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        outputs = self.model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state[:, 0, :]  # CLS pooling\n",
        "        return F.normalize(embeddings, dim=-1)\n",
        "\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    \"\"\"Projection layer Ï†: encoder_dim â†’ decoder_dim\"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim: int, decoder_dim: int):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(encoder_dim, decoder_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(decoder_dim, decoder_dim),\n",
        "        )\n",
        "        self.scale = nn.Parameter(torch.ones(1))\n",
        "        self.shift = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, target_stats: Optional[Tuple[float, float]] = None) -> torch.Tensor:\n",
        "        out = self.projection(x)\n",
        "        out = out * self.scale + self.shift\n",
        "        if target_stats is not None:\n",
        "            target_mean, target_std = target_stats\n",
        "            out_mean = out.mean()\n",
        "            out_std = out.std() + 1e-8\n",
        "            out = (out - out_mean) / out_std * target_std + target_mean\n",
        "        return out\n",
        "\n",
        "\n",
        "class ExpansionPolicy(nn.Module):\n",
        "    \"\"\"RL policy network for selective chunk expansion.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 256):\n",
        "        super().__init__()\n",
        "        self.embedding_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=hidden_dim, nhead=4, dim_feedforward=hidden_dim * 4,\n",
        "                dropout=0.1, batch_first=True\n",
        "            ),\n",
        "            num_layers=2\n",
        "        )\n",
        "        self.output_head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, chunk_embeddings: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        squeeze = False\n",
        "        if chunk_embeddings.dim() == 2:\n",
        "            chunk_embeddings = chunk_embeddings.unsqueeze(0)\n",
        "            squeeze = True\n",
        "\n",
        "        x = self.embedding_proj(chunk_embeddings)\n",
        "        x = self.transformer(x, src_key_padding_mask=mask)\n",
        "        logits = self.output_head(x).squeeze(-1)\n",
        "\n",
        "        if squeeze:\n",
        "            logits = logits.squeeze(0)\n",
        "        return logits\n",
        "\n",
        "    def sample_expansion_mask(self, chunk_embeddings: torch.Tensor, max_expand_fraction: float = 0.1\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Sample which chunks to expand using the policy.\"\"\"\n",
        "        logits = self.forward(chunk_embeddings)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        dist = torch.distributions.Bernoulli(probs=probs)\n",
        "        samples = dist.sample()\n",
        "        log_probs = dist.log_prob(samples)\n",
        "\n",
        "        num_chunks = len(logits)\n",
        "        max_expand = max(1, int(max_expand_fraction * num_chunks))\n",
        "\n",
        "        if samples.sum() > max_expand:\n",
        "            _, top_indices = torch.topk(logits, k=max_expand)\n",
        "            new_samples = torch.zeros_like(samples)\n",
        "            new_samples[top_indices] = 1\n",
        "            samples = new_samples\n",
        "\n",
        "        return samples.bool(), log_probs.sum()\n",
        "\n",
        "\n",
        "print(\"âœ… Model components loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ¤– Full REFRAG Model { display-mode: \"form\" }\n",
        "# @markdown Complete REFRAG model with training and generation methods\n",
        "\n",
        "class REFRAGModel(nn.Module):\n",
        "    \"\"\"Full REFRAG model implementing compress â†’ sense â†’ expand.\"\"\"\n",
        "\n",
        "    def __init__(self, config: REFRAGConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.device = get_device()\n",
        "\n",
        "        # Initialize components\n",
        "        print(f\"ðŸ“¥ Loading encoder: {config.encoder_name}\")\n",
        "        self.encoder = ChunkEncoder(config.encoder_name)\n",
        "\n",
        "        print(f\"ðŸ“¥ Loading decoder: {config.decoder_name}\")\n",
        "        self.decoder_tokenizer = AutoTokenizer.from_pretrained(config.decoder_name)\n",
        "        if self.decoder_tokenizer.pad_token is None:\n",
        "            self.decoder_tokenizer.pad_token = self.decoder_tokenizer.eos_token\n",
        "\n",
        "        # Determine dtype\n",
        "        use_fp16 = config.fp16 and self.device.type not in ['mps', 'cpu']\n",
        "        self.decoder = AutoModelForCausalLM.from_pretrained(\n",
        "            config.decoder_name,\n",
        "            torch_dtype=torch.float16 if use_fp16 else torch.float32,\n",
        "        )\n",
        "\n",
        "        self.decoder_embed_dim = self.decoder.get_input_embeddings().weight.shape[1]\n",
        "        self.projector = ProjectionLayer(self.encoder.hidden_size, self.decoder_embed_dim)\n",
        "        self.policy = ExpansionPolicy(self.encoder.hidden_size, config.policy_hidden_dim)\n",
        "\n",
        "        self.eos_token_id = self.decoder_tokenizer.eos_token_id\n",
        "        self.pad_token_id = self.decoder_tokenizer.pad_token_id\n",
        "\n",
        "    def to(self, device):\n",
        "        \"\"\"Move model to device.\"\"\"\n",
        "        self.device = device\n",
        "        self.encoder = self.encoder.to(device)\n",
        "        self.decoder = self.decoder.to(device)\n",
        "        self.projector = self.projector.to(device)\n",
        "        self.policy = self.policy.to(device)\n",
        "        return self\n",
        "\n",
        "    def freeze_decoder(self):\n",
        "        for param in self.decoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"ðŸ”’ Decoder frozen\")\n",
        "\n",
        "    def unfreeze_decoder(self):\n",
        "        for param in self.decoder.parameters():\n",
        "            param.requires_grad = True\n",
        "        print(\"ðŸ”“ Decoder unfrozen\")\n",
        "\n",
        "    def freeze_encoder_projector(self):\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.projector.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"ðŸ”’ Encoder and projector frozen\")\n",
        "\n",
        "    def _tokenize_text(self, text: str, max_length: int) -> torch.Tensor:\n",
        "        tokens = self.decoder_tokenizer(text, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "        return tokens.input_ids.to(self.device)\n",
        "\n",
        "    def _get_decoder_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        return self.decoder.get_input_embeddings()(input_ids)\n",
        "\n",
        "    def _chunk_tokens(self, input_ids: torch.Tensor, k: int) -> List[torch.Tensor]:\n",
        "        ids = input_ids.squeeze(0) if input_ids.dim() > 1 else input_ids\n",
        "        return [ids[i:i+k] for i in range(0, len(ids), k)]\n",
        "\n",
        "    def _chunks_to_text(self, chunks: List[torch.Tensor]) -> List[str]:\n",
        "        return [self.decoder_tokenizer.decode(c, skip_special_tokens=True) for c in chunks]\n",
        "\n",
        "    def compute_reconstruction_loss(self, text: str, num_chunks_cap: Optional[int] = None) -> torch.Tensor:\n",
        "        \"\"\"Compute reconstruction loss for Stage 1 training.\"\"\"\n",
        "        k = self.config.chunk_size_k\n",
        "        input_ids = self._tokenize_text(text, self.config.max_context_tokens)\n",
        "        chunks = self._chunk_tokens(input_ids, k)\n",
        "        if num_chunks_cap is not None:\n",
        "            chunks = chunks[:num_chunks_cap]\n",
        "\n",
        "        if len(chunks) == 0:\n",
        "            return torch.tensor(0.0, device=self.device, requires_grad=True)\n",
        "\n",
        "        chunk_texts = self._chunks_to_text(chunks)\n",
        "        chunk_embeddings = self.encoder(chunk_texts, device=self.device)\n",
        "        projected = self.projector(chunk_embeddings)\n",
        "\n",
        "        total_loss = 0.0\n",
        "        num_valid = 0\n",
        "\n",
        "        for i, chunk_ids in enumerate(chunks):\n",
        "            if len(chunk_ids) < 2:\n",
        "                continue\n",
        "\n",
        "            compressed_emb = projected[i:i+1, :]\n",
        "            target_ids = chunk_ids.to(self.device)\n",
        "            target_embs = self._get_decoder_embeddings(target_ids[:-1].unsqueeze(0))\n",
        "\n",
        "            decoder_dtype = next(self.decoder.parameters()).dtype\n",
        "            emb_to_concat = compressed_emb.unsqueeze(1)\n",
        "            if decoder_dtype == torch.float16:\n",
        "                emb_to_concat = emb_to_concat.clamp(-65000, 65000)\n",
        "            emb_to_concat = emb_to_concat.to(dtype=decoder_dtype)\n",
        "\n",
        "            input_embs = torch.cat([emb_to_concat, target_embs], dim=1)\n",
        "            labels = torch.cat([torch.tensor([-100], device=self.device), target_ids]).unsqueeze(0)\n",
        "            labels = labels[:, :input_embs.size(1)]\n",
        "\n",
        "            outputs = self.decoder(inputs_embeds=input_embs, labels=labels)\n",
        "            total_loss += outputs.loss\n",
        "            num_valid += 1\n",
        "\n",
        "        return total_loss / max(num_valid, 1)\n",
        "\n",
        "    def compute_cpt_loss(self, text: str, s: int = 128, o: int = 128, expand_fraction: float = 0.0) -> torch.Tensor:\n",
        "        \"\"\"Compute next-paragraph prediction loss for CPT.\"\"\"\n",
        "        k = self.config.chunk_size_k\n",
        "        input_ids = self._tokenize_text(text, s + o)\n",
        "        ids = input_ids.squeeze(0)\n",
        "\n",
        "        total_len = len(ids)\n",
        "        if total_len < k + 4:\n",
        "            return torch.tensor(0.0, device=self.device, requires_grad=True)\n",
        "\n",
        "        actual_s = min(s, total_len // 2)\n",
        "        actual_o = min(o, total_len - actual_s)\n",
        "        actual_s = max(actual_s, k)\n",
        "        actual_o = max(actual_o, 2)\n",
        "\n",
        "        if actual_s + actual_o > total_len:\n",
        "            actual_s = total_len - actual_o\n",
        "\n",
        "        context_ids = ids[:actual_s]\n",
        "        output_ids = ids[actual_s:actual_s + actual_o]\n",
        "\n",
        "        if len(output_ids) < 2 or len(context_ids) < k:\n",
        "            return torch.tensor(0.0, device=self.device, requires_grad=True)\n",
        "\n",
        "        context_chunks = self._chunk_tokens(context_ids, k)\n",
        "        chunk_texts = self._chunks_to_text(context_chunks)\n",
        "        chunk_embeddings = self.encoder(chunk_texts, device=self.device)\n",
        "        projected = self.projector(chunk_embeddings)\n",
        "\n",
        "        if torch.isnan(projected).any() or torch.isinf(projected).any():\n",
        "            return torch.tensor(0.0, device=self.device, requires_grad=True)\n",
        "\n",
        "        context_embs = projected.unsqueeze(0)\n",
        "        output_embs = self._get_decoder_embeddings(output_ids[:-1].unsqueeze(0).to(self.device))\n",
        "\n",
        "        decoder_dtype = next(self.decoder.parameters()).dtype\n",
        "        if decoder_dtype == torch.float16:\n",
        "            context_embs = context_embs.clamp(-65000, 65000)\n",
        "        context_embs = context_embs.to(dtype=decoder_dtype)\n",
        "\n",
        "        full_input = torch.cat([context_embs, output_embs], dim=1)\n",
        "        context_len = context_embs.size(1)\n",
        "        labels = torch.cat([\n",
        "            torch.full((context_len,), -100, device=self.device),\n",
        "            output_ids.to(self.device)\n",
        "        ]).unsqueeze(0)\n",
        "        labels = labels[:, :full_input.size(1)]\n",
        "\n",
        "        outputs = self.decoder(inputs_embeds=full_input, labels=labels)\n",
        "        return outputs.loss\n",
        "\n",
        "    def generate(self, question: str, passages: List[str], max_new_tokens: int = 256,\n",
        "                 temperature: float = 0.0, use_policy: bool = True, expand_fraction: float = 0.1) -> Dict[str, Any]:\n",
        "        \"\"\"Generate answer using REFRAG.\"\"\"\n",
        "        self.eval()\n",
        "        k = self.config.chunk_size_k\n",
        "\n",
        "        context_text = \" \".join(passages)\n",
        "        context_ids = self._tokenize_text(context_text, self.config.max_context_tokens)\n",
        "        chunks = self._chunk_tokens(context_ids, k)\n",
        "        chunk_texts = self._chunks_to_text(chunks)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            chunk_embeddings = self.encoder(chunk_texts, device=self.device)\n",
        "            projected = self.projector(chunk_embeddings)\n",
        "\n",
        "        L = len(chunks)\n",
        "        if use_policy and L > 0:\n",
        "            expand_mask, _ = self.policy.sample_expansion_mask(chunk_embeddings, expand_fraction)\n",
        "        else:\n",
        "            expand_mask = torch.zeros(L, dtype=torch.bool, device=self.device)\n",
        "\n",
        "        input_parts = []\n",
        "        for i, chunk_ids in enumerate(chunks):\n",
        "            if expand_mask[i] if L > 0 else False:\n",
        "                chunk_embs = self._get_decoder_embeddings(chunk_ids.unsqueeze(0).to(self.device))\n",
        "                input_parts.append(chunk_embs.squeeze(0))\n",
        "            else:\n",
        "                if L > 0:\n",
        "                    input_parts.append(projected[i:i+1, :])\n",
        "\n",
        "        if input_parts:\n",
        "            context_embs = torch.cat(input_parts, dim=0).unsqueeze(0)\n",
        "        else:\n",
        "            context_embs = torch.zeros(1, 0, self.decoder_embed_dim, device=self.device)\n",
        "\n",
        "        q_prompt = f\"Question: {question}\\nAnswer:\"\n",
        "        q_ids = self._tokenize_text(q_prompt, self.config.max_query_tokens)\n",
        "        q_embs = self._get_decoder_embeddings(q_ids)\n",
        "\n",
        "        full_input = torch.cat([context_embs, q_embs], dim=1) if context_embs.size(1) > 0 else q_embs\n",
        "\n",
        "        t0 = time.time()\n",
        "        outputs = self.decoder(inputs_embeds=full_input, use_cache=True)\n",
        "        past_kv = outputs.past_key_values\n",
        "        ttft = time.time() - t0\n",
        "\n",
        "        generated = []\n",
        "        ttit_times = []\n",
        "        current = torch.tensor([[self.eos_token_id]], device=self.device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            step_emb = self._get_decoder_embeddings(current)\n",
        "            t1 = time.time()\n",
        "            out = self.decoder(inputs_embeds=step_emb, past_key_values=past_kv, use_cache=True)\n",
        "            ttit_times.append(time.time() - t1)\n",
        "            past_kv = out.past_key_values\n",
        "\n",
        "            if temperature > 0:\n",
        "                probs = F.softmax(out.logits[:, -1, :] / temperature, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                next_token = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "\n",
        "            if next_token.item() == self.eos_token_id:\n",
        "                break\n",
        "\n",
        "            generated.append(next_token.item())\n",
        "            current = next_token\n",
        "\n",
        "        answer = self.decoder_tokenizer.decode(generated, skip_special_tokens=True)\n",
        "\n",
        "        return {\n",
        "            'answer': answer.strip(),\n",
        "            'question': question,\n",
        "            'num_passages': len(passages),\n",
        "            'num_chunks': L,\n",
        "            'num_expanded': expand_mask.sum().item() if L > 0 else 0,\n",
        "            'compression_rate': k,\n",
        "            'ttft_sec': ttft,\n",
        "            'ttit_avg_sec': np.mean(ttit_times) if ttit_times else 0,\n",
        "            'throughput_tok_per_sec': len(generated) / sum(ttit_times) if ttit_times else 0,\n",
        "            'generated_tokens': len(generated),\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, path: str, optimizer=None, scheduler=None, step: int = 0):\n",
        "        \"\"\"Save model checkpoint.\"\"\"\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        checkpoint = {\n",
        "            'config': self.config.to_dict(),\n",
        "            'encoder_state_dict': self.encoder.state_dict(),\n",
        "            'projector_state_dict': self.projector.state_dict(),\n",
        "            'policy_state_dict': self.policy.state_dict(),\n",
        "            'step': step,\n",
        "        }\n",
        "        self.decoder.save_pretrained(os.path.join(path, 'decoder'))\n",
        "        if optimizer is not None:\n",
        "            checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "        if scheduler is not None:\n",
        "            checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
        "        torch.save(checkpoint, os.path.join(path, 'checkpoint.pt'))\n",
        "        print(f\"ðŸ’¾ Saved checkpoint to {path}\")\n",
        "\n",
        "    def load_checkpoint(self, path: str, load_decoder: bool = True):\n",
        "        \"\"\"Load model checkpoint.\"\"\"\n",
        "        checkpoint = torch.load(os.path.join(path, 'checkpoint.pt'), map_location=self.device)\n",
        "        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "        self.projector.load_state_dict(checkpoint['projector_state_dict'], strict=False)\n",
        "        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
        "\n",
        "        if load_decoder:\n",
        "            decoder_path = os.path.join(path, 'decoder')\n",
        "            if os.path.exists(decoder_path):\n",
        "                self.decoder = AutoModelForCausalLM.from_pretrained(\n",
        "                    decoder_path,\n",
        "                    torch_dtype=torch.float16 if self.config.fp16 else torch.float32,\n",
        "                ).to(self.device)\n",
        "        print(f\"âœ… Loaded checkpoint from {path}\")\n",
        "        return checkpoint.get('step', 0)\n",
        "\n",
        "\n",
        "print(\"âœ… REFRAG model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ‹ï¸ Training Functions { display-mode: \"form\" }\n",
        "# @markdown Training loops for Stage 1 (Reconstruction), Stage 2 (CPT), and Stage 3 (Policy)\n",
        "\n",
        "def train_reconstruction(model: REFRAGModel, config: REFRAGConfig, data_path: str, output_dir: str):\n",
        "    \"\"\"Stage 1: Reconstruction training with curriculum learning.\"\"\"\n",
        "    device = get_device()\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ðŸ‹ï¸ Stage 1: Reconstruction Training\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Config: lr={config.lr_reconstruction}, batch={config.batch_size}, stages={config.num_curriculum_stages}\")\n",
        "\n",
        "    model.freeze_decoder()\n",
        "    model.train()\n",
        "\n",
        "    dataset = CPTDataset(data_path, model.decoder_tokenizer)\n",
        "    params = list(model.encoder.parameters()) + list(model.projector.parameters())\n",
        "    optimizer = torch.optim.AdamW(params, lr=config.lr_reconstruction, weight_decay=config.weight_decay)\n",
        "    schedule = get_curriculum_schedule(config.num_curriculum_stages)\n",
        "\n",
        "    global_step = 0\n",
        "    losses = []\n",
        "\n",
        "    for stage_config in schedule:\n",
        "        stage = stage_config['stage']\n",
        "        max_chunks = stage_config['max_chunks']\n",
        "\n",
        "        print(f\"\\n--- Stage {stage}/{config.num_curriculum_stages} (max_chunks={max_chunks}) ---\")\n",
        "        steps_per_stage = len(dataset) * config.epochs_per_stage // config.batch_size\n",
        "\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=int(config.warmup_ratio * steps_per_stage),\n",
        "            num_training_steps=steps_per_stage\n",
        "        )\n",
        "\n",
        "        running_loss = 0.0\n",
        "        step_count = 0\n",
        "\n",
        "        for epoch in range(config.epochs_per_stage):\n",
        "            indices = list(range(len(dataset)))\n",
        "            random.shuffle(indices)\n",
        "\n",
        "            pbar = tqdm(range(0, len(indices), config.batch_size), desc=f\"Stage {stage}\", leave=False)\n",
        "            for i in pbar:\n",
        "                batch_indices = indices[i:i + config.batch_size]\n",
        "                optimizer.zero_grad()\n",
        "                batch_loss = torch.tensor(0.0, device=device, requires_grad=False)\n",
        "                valid_items = 0\n",
        "\n",
        "                for idx in batch_indices:\n",
        "                    item = dataset[idx]\n",
        "                    num_chunks = sample_num_chunks_for_stage(stage_config)\n",
        "                    num_chunks = min(num_chunks, max_chunks)\n",
        "\n",
        "                    loss = model.compute_reconstruction_loss(item['text'], num_chunks_cap=num_chunks)\n",
        "\n",
        "                    if loss.requires_grad:\n",
        "                        scaled_loss = loss / len(batch_indices)\n",
        "                        scaled_loss.backward()\n",
        "                        batch_loss = batch_loss + loss.detach()\n",
        "                        valid_items += 1\n",
        "\n",
        "                if valid_items > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(params, config.max_grad_norm)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "\n",
        "                    running_loss += (batch_loss / valid_items).item()\n",
        "                    step_count += 1\n",
        "                    global_step += 1\n",
        "                    losses.append((batch_loss / valid_items).item())\n",
        "\n",
        "                    if global_step % 10 == 0:\n",
        "                        avg_loss = running_loss / step_count\n",
        "                        lr = scheduler.get_last_lr()[0]\n",
        "                        pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.2e}'})\n",
        "                        running_loss = 0.0\n",
        "                        step_count = 0\n",
        "\n",
        "    model.save_checkpoint(output_dir, optimizer, step=global_step)\n",
        "    return global_step, losses\n",
        "\n",
        "\n",
        "def train_cpt(model: REFRAGModel, config: REFRAGConfig, data_path: str, output_dir: str):\n",
        "    \"\"\"Stage 2: Continual Pre-Training (next-paragraph prediction).\"\"\"\n",
        "    device = get_device()\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ðŸ‹ï¸ Stage 2: Continual Pre-Training (CPT)\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Config: lr={config.lr_cpt}, lr_decoder={config.lr_cpt_decoder}, batch={config.batch_size}\")\n",
        "\n",
        "    model.unfreeze_decoder()\n",
        "    model.train()\n",
        "\n",
        "    dataset = CPTDataset(data_path, model.decoder_tokenizer)\n",
        "\n",
        "    # Freeze encoder/projector, train only decoder\n",
        "    for param in model.encoder.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.projector.parameters():\n",
        "        param.requires_grad = False\n",
        "    print(\"ðŸ”’ Encoder/Projector frozen for CPT\")\n",
        "\n",
        "    param_groups = [\n",
        "        {'params': model.decoder.parameters(), 'lr': config.lr_cpt_decoder},\n",
        "        {'params': model.policy.parameters(), 'lr': config.lr_cpt}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(param_groups, weight_decay=config.weight_decay)\n",
        "\n",
        "    schedule = get_curriculum_schedule(config.num_curriculum_stages)\n",
        "    global_step = 0\n",
        "    losses = []\n",
        "\n",
        "    for stage_config in schedule:\n",
        "        stage = stage_config['stage']\n",
        "        expand_frac = min(0.5, stage * 0.05)\n",
        "\n",
        "        print(f\"\\n--- Stage {stage}/{config.num_curriculum_stages} (expand_frac={expand_frac:.2f}) ---\")\n",
        "        steps_per_stage = len(dataset) * config.epochs_per_stage // config.batch_size\n",
        "\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=int(config.warmup_ratio * steps_per_stage),\n",
        "            num_training_steps=steps_per_stage\n",
        "        )\n",
        "\n",
        "        running_loss = 0.0\n",
        "        step_count = 0\n",
        "\n",
        "        for epoch in range(config.epochs_per_stage):\n",
        "            indices = list(range(len(dataset)))\n",
        "            random.shuffle(indices)\n",
        "\n",
        "            pbar = tqdm(range(0, len(indices), config.batch_size), desc=f\"CPT Stage {stage}\", leave=False)\n",
        "            for i in pbar:\n",
        "                batch_indices = indices[i:i + config.batch_size]\n",
        "                optimizer.zero_grad()\n",
        "                batch_loss = torch.tensor(0.0, device=device, requires_grad=False)\n",
        "                valid_items = 0\n",
        "\n",
        "                for idx in batch_indices:\n",
        "                    item = dataset[idx]\n",
        "                    s = config.max_context_tokens\n",
        "                    o = config.max_output_tokens\n",
        "\n",
        "                    loss = model.compute_cpt_loss(item['text'], s=s, o=o, expand_fraction=expand_frac)\n",
        "\n",
        "                    if loss.requires_grad and not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "                        scaled_loss = loss / len(batch_indices)\n",
        "                        scaled_loss.backward()\n",
        "                        batch_loss = batch_loss + loss.detach()\n",
        "                        valid_items += 1\n",
        "\n",
        "                if valid_items > 0:\n",
        "                    # Check for NaN gradients\n",
        "                    has_nan_grad = False\n",
        "                    for param in model.parameters():\n",
        "                        if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
        "                            has_nan_grad = True\n",
        "                            break\n",
        "\n",
        "                    if has_nan_grad:\n",
        "                        print(f\"âš ï¸ NaN gradients at step {global_step}, skipping\")\n",
        "                        optimizer.zero_grad()\n",
        "                        continue\n",
        "\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "\n",
        "                    running_loss += (batch_loss / valid_items).item()\n",
        "                    step_count += 1\n",
        "                    global_step += 1\n",
        "                    losses.append((batch_loss / valid_items).item())\n",
        "\n",
        "                    if global_step % 10 == 0:\n",
        "                        avg_loss = running_loss / step_count\n",
        "                        lr = scheduler.get_last_lr()[0]\n",
        "                        pbar.set_postfix({'loss': f'{avg_loss:.4f}', 'lr': f'{lr:.2e}'})\n",
        "                        running_loss = 0.0\n",
        "                        step_count = 0\n",
        "\n",
        "    model.save_checkpoint(output_dir, optimizer, step=global_step)\n",
        "    print(\"âœ… CPT training complete!\")\n",
        "    return global_step, losses\n",
        "\n",
        "\n",
        "print(\"âœ… Training functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸš€ Training Pipeline\n",
        "\n",
        "Now let's run the actual training pipeline. Execute the cells below in order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ“Š Stage 0: Build FAISS Index { display-mode: \"form\" }\n",
        "# @markdown Build the FAISS index from corpus for retrieval\n",
        "\n",
        "# Check if corpus exists\n",
        "if not os.path.exists(CORPUS_FILE):\n",
        "    print(f\"âŒ Corpus file not found: {CORPUS_FILE}\")\n",
        "    print(\"   Please run the data generation cell first!\")\n",
        "else:\n",
        "    # Load passages\n",
        "    with open(CORPUS_FILE, 'r', encoding='utf-8') as f:\n",
        "        passages = [line.strip() for line in f if line.strip()]\n",
        "    \n",
        "    print(f\"ðŸ“š Loaded {len(passages)} passages from {CORPUS_FILE}\")\n",
        "    \n",
        "    # Check if index already exists\n",
        "    index_path = os.path.join(INDEX_DIR, 'faiss.index')\n",
        "    if os.path.exists(index_path):\n",
        "        print(f\"âš ï¸ Index already exists at {index_path}\")\n",
        "        rebuild = input(\"Rebuild? (y/n): \").lower().strip() == 'y'\n",
        "    else:\n",
        "        rebuild = True\n",
        "    \n",
        "    if rebuild:\n",
        "        print(\"\\nðŸ”§ Building FAISS index...\")\n",
        "        retriever = PassageRetriever(EMBED_MODEL)\n",
        "        retriever.build_index(passages, index_path)\n",
        "        print(f\"âœ… Index saved to {INDEX_DIR}\")\n",
        "    else:\n",
        "        print(\"â„¹ï¸ Using existing index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ”¬ Initialize REFRAG Model { display-mode: \"form\" }\n",
        "# @markdown Create the REFRAG model with configured settings\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed_everything(SEED)\n",
        "\n",
        "# Create config\n",
        "config = REFRAGConfig(\n",
        "    encoder_name=ENCODER_MODEL,\n",
        "    decoder_name=DECODER_MODEL,\n",
        "    chunk_size_k=K,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_curriculum_stages=NUM_CURRICULUM_STAGES,\n",
        "    lr_reconstruction=LR_RECONSTRUCTION,\n",
        "    lr_cpt=LR_CPT,\n",
        "    lr_cpt_decoder=LR_CPT_DECODER,\n",
        "    fp16=USE_FP16,\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "print(\"ðŸ”§ Creating REFRAG model...\")\n",
        "print(\"   This may take a few minutes to download model weights...\")\n",
        "\n",
        "device = get_device()\n",
        "model = REFRAGModel(config).to(device)\n",
        "\n",
        "# Print model info\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nðŸ“Š Model Statistics:\")\n",
        "print(f\"   Total parameters: {total_params / 1e6:.1f}M\")\n",
        "print(f\"   Trainable parameters: {trainable_params / 1e6:.1f}M\")\n",
        "print(f\"   Device: {device}\")\n",
        "\n",
        "# Check GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1e9\n",
        "    reserved = torch.cuda.memory_reserved() / 1e9\n",
        "    print(f\"   GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ‹ï¸ Stage 1: Reconstruction Training { display-mode: \"form\" }\n",
        "# @markdown Train encoder + projector (decoder frozen)\n",
        "# @markdown This is the first phase where we learn to compress tokens into embeddings\n",
        "\n",
        "# Check for existing checkpoint\n",
        "recon_checkpoint = os.path.join(RECON_DIR, 'checkpoint.pt')\n",
        "if os.path.exists(recon_checkpoint):\n",
        "    print(f\"âš ï¸ Checkpoint exists at {RECON_DIR}\")\n",
        "    retrain = input(\"Retrain from scratch? (y/n): \").lower().strip() == 'y'\n",
        "    if not retrain:\n",
        "        print(\"â„¹ï¸ Loading existing checkpoint...\")\n",
        "        model.load_checkpoint(RECON_DIR, load_decoder=False)\n",
        "        print(\"âœ… Checkpoint loaded, skipping Stage 1\")\n",
        "    else:\n",
        "        print(\"ðŸ”„ Retraining from scratch...\")\n",
        "        import shutil\n",
        "        shutil.rmtree(RECON_DIR, ignore_errors=True)\n",
        "        os.makedirs(RECON_DIR, exist_ok=True)\n",
        "        \n",
        "        # Run training\n",
        "        global_step, losses = train_reconstruction(model, config, CPT_TRAIN_FILE, RECON_DIR)\n",
        "        print(f\"\\nâœ… Stage 1 Complete! Steps: {global_step}\")\n",
        "else:\n",
        "    print(\"ðŸš€ Starting Stage 1: Reconstruction Training...\")\n",
        "    global_step, losses = train_reconstruction(model, config, CPT_TRAIN_FILE, RECON_DIR)\n",
        "    print(f\"\\nâœ… Stage 1 Complete! Steps: {global_step}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ‹ï¸ Stage 2: Continual Pre-Training (CPT) { display-mode: \"form\" }\n",
        "# @markdown Train decoder to predict next paragraph from compressed context\n",
        "# @markdown This phase unfreezes the decoder and fine-tunes it\n",
        "\n",
        "# First, ensure Stage 1 checkpoint exists\n",
        "recon_checkpoint = os.path.join(RECON_DIR, 'checkpoint.pt')\n",
        "if not os.path.exists(recon_checkpoint):\n",
        "    print(\"âŒ Stage 1 checkpoint not found!\")\n",
        "    print(\"   Please run Stage 1 (Reconstruction Training) first.\")\n",
        "else:\n",
        "    # Check for existing CPT checkpoint\n",
        "    cpt_checkpoint = os.path.join(CPT_DIR, 'checkpoint.pt')\n",
        "    if os.path.exists(cpt_checkpoint):\n",
        "        print(f\"âš ï¸ CPT Checkpoint exists at {CPT_DIR}\")\n",
        "        retrain = input(\"Retrain from scratch? (y/n): \").lower().strip() == 'y'\n",
        "        if not retrain:\n",
        "            print(\"â„¹ï¸ Loading existing CPT checkpoint...\")\n",
        "            model.load_checkpoint(CPT_DIR)\n",
        "            print(\"âœ… CPT checkpoint loaded, skipping Stage 2\")\n",
        "        else:\n",
        "            print(\"ðŸ”„ Retraining CPT from Stage 1 checkpoint...\")\n",
        "            import shutil\n",
        "            shutil.rmtree(CPT_DIR, ignore_errors=True)\n",
        "            os.makedirs(CPT_DIR, exist_ok=True)\n",
        "            \n",
        "            # Reload Stage 1 checkpoint\n",
        "            model.load_checkpoint(RECON_DIR)\n",
        "            \n",
        "            # Run CPT training\n",
        "            global_step, losses = train_cpt(model, config, CPT_TRAIN_FILE, CPT_DIR)\n",
        "            print(f\"\\nâœ… Stage 2 (CPT) Complete! Steps: {global_step}\")\n",
        "    else:\n",
        "        print(\"ðŸš€ Starting Stage 2: Continual Pre-Training...\")\n",
        "        \n",
        "        # Load Stage 1 checkpoint first\n",
        "        print(\"ðŸ“¥ Loading Stage 1 checkpoint...\")\n",
        "        model.load_checkpoint(RECON_DIR)\n",
        "        \n",
        "        # Run CPT training\n",
        "        global_step, losses = train_cpt(model, config, CPT_TRAIN_FILE, CPT_DIR)\n",
        "        print(f\"\\nâœ… Stage 2 (CPT) Complete! Steps: {global_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª Testing and Evaluation\n",
        "\n",
        "Now let's test the trained model with generation and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸŽ¯ Test Generation { display-mode: \"form\" }\n",
        "# @markdown Test the trained model with a sample question\n",
        "\n",
        "# Load the best checkpoint\n",
        "best_checkpoint = CPT_DIR if os.path.exists(os.path.join(CPT_DIR, 'checkpoint.pt')) else RECON_DIR\n",
        "\n",
        "if os.path.exists(os.path.join(best_checkpoint, 'checkpoint.pt')):\n",
        "    print(f\"ðŸ“¥ Loading checkpoint from: {best_checkpoint}\")\n",
        "    model.load_checkpoint(best_checkpoint)\n",
        "    model.eval()\n",
        "    \n",
        "    # Load retriever\n",
        "    index_path = os.path.join(INDEX_DIR, 'faiss.index')\n",
        "    if os.path.exists(index_path):\n",
        "        print(\"ðŸ“¥ Loading FAISS index...\")\n",
        "        retriever = PassageRetriever(EMBED_MODEL)\n",
        "        retriever.load_index(index_path)\n",
        "        \n",
        "        # Test questions\n",
        "        test_questions = [\n",
        "            \"Which river flows through City_3?\",\n",
        "            \"What is the melting point of Alloy_11?\",\n",
        "            \"In which field does Person_13 work?\",\n",
        "        ]\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ðŸ§ª Testing Generation\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        for question in test_questions:\n",
        "            print(f\"\\nâ“ Question: {question}\")\n",
        "            \n",
        "            # Retrieve passages\n",
        "            results = retriever.search(question, top_k=TOP_K)\n",
        "            passages = [r[0] for r in results]\n",
        "            \n",
        "            # Generate answer\n",
        "            with torch.no_grad():\n",
        "                output = model.generate(\n",
        "                    question=question,\n",
        "                    passages=passages,\n",
        "                    max_new_tokens=64,\n",
        "                    temperature=0.0,\n",
        "                    use_policy=False,  # Disable policy for deterministic output\n",
        "                )\n",
        "            \n",
        "            print(f\"ðŸ’¬ Answer: {output['answer']}\")\n",
        "            print(f\"   â±ï¸ TTFT: {output['ttft_sec']:.3f}s | Tokens: {output['generated_tokens']}\")\n",
        "            print(f\"   ðŸ“Š Chunks: {output['num_chunks']} | Compression: {output['compression_rate']}Ã—\")\n",
        "    else:\n",
        "        print(f\"âŒ Index not found at {index_path}\")\n",
        "        print(\"   Please run Stage 0 (Build FAISS Index) first.\")\n",
        "else:\n",
        "    print(\"âŒ No checkpoint found!\")\n",
        "    print(\"   Please run the training stages first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ“Š Run Evaluation { display-mode: \"form\" }\n",
        "# @markdown Evaluate the model on the test set\n",
        "\n",
        "max_eval_samples = 50  # @param {type:\"slider\", min:10, max:200, step:10}\n",
        "\n",
        "# Check prerequisites\n",
        "best_checkpoint = CPT_DIR if os.path.exists(os.path.join(CPT_DIR, 'checkpoint.pt')) else RECON_DIR\n",
        "index_path = os.path.join(INDEX_DIR, 'faiss.index')\n",
        "\n",
        "if not os.path.exists(os.path.join(best_checkpoint, 'checkpoint.pt')):\n",
        "    print(\"âŒ No checkpoint found! Run training first.\")\n",
        "elif not os.path.exists(index_path):\n",
        "    print(\"âŒ Index not found! Run Stage 0 first.\")\n",
        "elif not os.path.exists(RAG_EVAL_FILE):\n",
        "    print(\"âŒ Evaluation data not found! Generate data first.\")\n",
        "else:\n",
        "    print(f\"ðŸ“¥ Loading checkpoint from: {best_checkpoint}\")\n",
        "    model.load_checkpoint(best_checkpoint)\n",
        "    model.eval()\n",
        "    \n",
        "    print(\"ðŸ“¥ Loading FAISS index...\")\n",
        "    retriever = PassageRetriever(EMBED_MODEL)\n",
        "    retriever.load_index(index_path)\n",
        "    \n",
        "    # Load eval data\n",
        "    eval_data = RAGDataset(RAG_EVAL_FILE)\n",
        "    \n",
        "    print(f\"\\nðŸ§ª Evaluating on {min(len(eval_data), max_eval_samples)} samples...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_ttft = 0.0\n",
        "    total_tokens = 0\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    pbar = tqdm(eval_data.data[:max_eval_samples], desc=\"Evaluating\")\n",
        "    for item in pbar:\n",
        "        question = item['question']\n",
        "        gold_answers = item.get('answers', [])\n",
        "        if isinstance(gold_answers, str):\n",
        "            gold_answers = [gold_answers]\n",
        "        \n",
        "        # Retrieve and generate\n",
        "        search_results = retriever.search(question, top_k=TOP_K)\n",
        "        passages = [r[0] for r in search_results]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                question=question,\n",
        "                passages=passages,\n",
        "                max_new_tokens=64,\n",
        "                temperature=0.0,\n",
        "                use_policy=False,\n",
        "            )\n",
        "        \n",
        "        predicted = output['answer'].lower().strip()\n",
        "        \n",
        "        # Check accuracy (substring match)\n",
        "        is_correct = any(\n",
        "            gold.lower().strip() in predicted or predicted in gold.lower().strip()\n",
        "            for gold in gold_answers\n",
        "        )\n",
        "        \n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        total_ttft += output['ttft_sec']\n",
        "        total_tokens += output['generated_tokens']\n",
        "        \n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'gold': gold_answers,\n",
        "            'predicted': output['answer'],\n",
        "            'correct': is_correct,\n",
        "        })\n",
        "        \n",
        "        pbar.set_postfix({'acc': f'{correct/total:.2%}'})\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ðŸ“Š Evaluation Results\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"   Accuracy: {correct}/{total} ({correct/total:.2%})\")\n",
        "    print(f\"   Avg TTFT: {total_ttft/total:.4f}s\")\n",
        "    print(f\"   Avg tokens: {total_tokens/total:.1f}\")\n",
        "    \n",
        "    # Show some examples\n",
        "    print(\"\\nðŸ“ Sample Results:\")\n",
        "    for i, r in enumerate(results[:5]):\n",
        "        status = \"âœ…\" if r['correct'] else \"âŒ\"\n",
        "        print(f\"   {status} Q: {r['question'][:50]}...\")\n",
        "        print(f\"      Gold: {r['gold'][0] if r['gold'] else 'N/A'}\")\n",
        "        print(f\"      Pred: {r['predicted'][:50]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ’¾ Save and Download Model\n",
        "\n",
        "Save the trained model and download it from Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ’¾ Save Model to Google Drive (Colab) { display-mode: \"form\" }\n",
        "# @markdown Mount Google Drive and save the model checkpoint\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    \n",
        "    # Mount Drive\n",
        "    print(\"ðŸ“ Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Create save directory\n",
        "    save_dir = '/content/drive/MyDrive/refrag_v2_checkpoints'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy checkpoints\n",
        "    import shutil\n",
        "    from datetime import datetime\n",
        "    \n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    \n",
        "    for src_dir, name in [(RECON_DIR, 'recon'), (CPT_DIR, 'cpt'), (INDEX_DIR, 'index')]:\n",
        "        if os.path.exists(src_dir):\n",
        "            dst_dir = os.path.join(save_dir, f'{name}_{timestamp}')\n",
        "            print(f\"ðŸ’¾ Copying {src_dir} -> {dst_dir}\")\n",
        "            shutil.copytree(src_dir, dst_dir)\n",
        "    \n",
        "    print(f\"\\nâœ… Model saved to Google Drive: {save_dir}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"â„¹ï¸ Not running in Colab. Checkpoints are saved locally in 'runs/' directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸ“¥ Download Model (Colab) { display-mode: \"form\" }\n",
        "# @markdown Download the model checkpoint as a zip file\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import shutil\n",
        "    import zipfile\n",
        "    \n",
        "    # Create a zip file of the checkpoints\n",
        "    zip_filename = 'refrag_v2_checkpoint.zip'\n",
        "    \n",
        "    print(\"ðŸ“¦ Creating zip archive...\")\n",
        "    \n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root_dir in [RECON_DIR, CPT_DIR, INDEX_DIR]:\n",
        "            if os.path.exists(root_dir):\n",
        "                for root, dirs, files_list in os.walk(root_dir):\n",
        "                    for file in files_list:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        arcname = os.path.relpath(file_path, os.path.dirname(root_dir))\n",
        "                        zipf.write(file_path, arcname)\n",
        "                        print(f\"   Added: {arcname}\")\n",
        "    \n",
        "    # Get file size\n",
        "    zip_size = os.path.getsize(zip_filename) / 1e6\n",
        "    print(f\"\\nðŸ“Š Zip file size: {zip_size:.1f} MB\")\n",
        "    \n",
        "    print(\"â¬‡ï¸ Starting download...\")\n",
        "    files.download(zip_filename)\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"â„¹ï¸ Not running in Colab. Checkpoints are in 'runs/' directory.\")\n",
        "    print(f\"   - Reconstruction: {RECON_DIR}\")\n",
        "    print(f\"   - CPT: {CPT_DIR}\")\n",
        "    print(f\"   - Index: {INDEX_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ® Interactive Demo\n",
        "\n",
        "Try the model with your own questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ðŸŽ® Interactive Question Answering { display-mode: \"form\" }\n",
        "# @markdown Enter your own question to test the model\n",
        "\n",
        "your_question = \"Which river flows through City_10?\"  # @param {type:\"string\"}\n",
        "max_tokens = 64  # @param {type:\"slider\", min:16, max:256, step:16}\n",
        "temperature = 0.0  # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "# Check prerequisites\n",
        "best_checkpoint = CPT_DIR if os.path.exists(os.path.join(CPT_DIR, 'checkpoint.pt')) else RECON_DIR\n",
        "index_path = os.path.join(INDEX_DIR, 'faiss.index')\n",
        "\n",
        "if not os.path.exists(os.path.join(best_checkpoint, 'checkpoint.pt')):\n",
        "    print(\"âŒ No checkpoint found! Run training first.\")\n",
        "elif not os.path.exists(index_path):\n",
        "    print(\"âŒ Index not found! Run Stage 0 first.\")\n",
        "else:\n",
        "    # Ensure model and retriever are loaded\n",
        "    if 'retriever' not in dir() or retriever is None:\n",
        "        print(\"ðŸ“¥ Loading retriever...\")\n",
        "        retriever = PassageRetriever(EMBED_MODEL)\n",
        "        retriever.load_index(index_path)\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"\\nâ“ Question: {your_question}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Retrieve passages\n",
        "    search_results = retriever.search(your_question, top_k=TOP_K)\n",
        "    passages = [r[0] for r in search_results]\n",
        "    \n",
        "    print(f\"ðŸ“š Retrieved {len(passages)} passages:\")\n",
        "    for i, (passage, score) in enumerate(search_results[:3]):\n",
        "        print(f\"   [{i+1}] (score: {score:.3f}) {passage[:80]}...\")\n",
        "    \n",
        "    # Generate answer\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            question=your_question,\n",
        "            passages=passages,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            use_policy=False,\n",
        "        )\n",
        "    \n",
        "    print(f\"\\nðŸ’¬ Answer: {output['answer']}\")\n",
        "    print(f\"\\nðŸ“Š Statistics:\")\n",
        "    print(f\"   â±ï¸ Time to first token: {output['ttft_sec']:.3f}s\")\n",
        "    print(f\"   ðŸ“ Generated tokens: {output['generated_tokens']}\")\n",
        "    print(f\"   ðŸ“¦ Chunks used: {output['num_chunks']}\")\n",
        "    print(f\"   ðŸ—œï¸ Compression rate: {output['compression_rate']}Ã—\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“– Appendix: Troubleshooting & Tips\n",
        "\n",
        "### Common Issues\n",
        "\n",
        "1. **Out of Memory (OOM)**\n",
        "   - Reduce `BATCH_SIZE` to 1-2\n",
        "   - Use a smaller decoder model (e.g., `TinyLlama/TinyLlama-1.1B-Chat-v1.0`)\n",
        "   - Reduce `NUM_CURRICULUM_STAGES` to 3-5\n",
        "   \n",
        "2. **NaN Loss during training**\n",
        "   - Set `USE_FP16 = False`\n",
        "   - Reduce learning rates\n",
        "   \n",
        "3. **Slow training**\n",
        "   - Ensure you're using a GPU (check Runtime > Change runtime type)\n",
        "   - Use Colab Pro for better GPUs (A100, V100)\n",
        "\n",
        "4. **HuggingFace authentication errors**\n",
        "   - Make sure you've accepted the model license on HuggingFace\n",
        "   - Try re-running the login cell\n",
        "\n",
        "### GPU Memory Guide\n",
        "\n",
        "| Model | GPU Memory | Recommended Batch Size |\n",
        "|-------|------------|------------------------|\n",
        "| TinyLlama-1.1B | ~4GB | 4-8 |\n",
        "| Llama-3.2-1B | ~6GB | 2-4 |\n",
        "| Llama-3.2-3B | ~12GB | 1-2 |\n",
        "| Phi-2 | ~8GB | 2-4 |\n",
        "\n",
        "### Paper Reference\n",
        "\n",
        "REFRAG: Rethinking RAG based Decoding  \n",
        "Meta Superintelligence Labs  \n",
        "arXiv:2509.01092v2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO9sGTpWocbCrt98RYqwUkG",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
