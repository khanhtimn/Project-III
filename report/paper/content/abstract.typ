Các Mô hình Ngôn ngữ Lớn (LLM) đã thể hiện năng lực đáng kể trong việc tận dụng tri thức bên ngoài để nâng cao chất lượng trả lời trong các ứng dụng nhiều lượt hội thoại và dạng tác tử (agentic), điển hình là các hệ thống sinh văn bản tăng cường truy hồi (Retrieval-Augmented Generation -- RAG). Tuy nhiên, xử lý ngữ cảnh đầu vào rất dài gây ra độ trễ hệ thống lớn và yêu cầu bộ nhớ đáng kể cho bộ nhớ khoá–giá trị (KV cache), làm giảm thông lượng suy luận và tạo ra một đánh đổi cơ bản giữa việc làm giàu tri thức và hiệu năng hệ thống.

Trong các hệ thống RAG, phần lớn ngữ cảnh mà LLM nhận được là chuỗi các đoạn văn được ghép nối từ bộ truy hồi, trong đó chỉ một phần nhỏ thực sự liên quan trực tiếp tới truy vấn. Các đoạn này thường có độ tương tự ngữ nghĩa thấp do cơ chế đa dạng hoá hoặc khử trùng lặp trong bước xếp hạng lại, dẫn tới mẫu hình chú ý dạng khối chéo, khác với các tác vụ sinh văn bản tiêu chuẩn.

Từ quan sát này, có thể thấy phần lớn phép tính trên ngữ cảnh RAG trong giai đoạn giải mã là không cần thiết và có thể loại bỏ với mức suy giảm hiệu năng rất nhỏ.

Em đề xuất một khung giải mã hiệu quả dựa trên việc nén, cảm nhận và giãn nở biểu diễn ngữ cảnh để giảm độ trễ trong các ứng dụng RAG. Bằng cách khai thác độ thưa (sparsity) trong cấu trúc chú ý, khung RAG mới giảm đáng kể độ trễ (đặc biệt là thời gian tạo token đầu tiên), bộ nhớ KV cache và chi phí tính toán, mà không cần sửa kiến trúc LLM gốc. Thực nghiệm cho thấy mẫu RAG này đạt tới 30.85× tăng tốc thời gian tạo token đầu tiên (cao hơn #(3.75)× so với phương pháp trước đó). Hơn nữa, tối ưu hoá ngữ cảnh cho phép khung RAG này mở rộng hiệu quả cửa sổ ngữ cảnh của LLM lên tới #(16)×. Thử nghiệm rộng trên RAG, hội thoại nhiều lượt và tóm tắt tài liệu dài cho thấy khung RAG này đem lại tăng tốc đáng kể mà không suy giảm độ chính xác so với các mô hình LLaMA và các mô hình gốc hiện đại khác.